{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fraud_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMasvBWxOndNSHRYnnZjzLy",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pfolaa/dsti-labs/blob/main/fraud_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfrluZHFeoJi",
        "outputId": "f83958de-904e-4a4b-94e0-7ece8c739a28"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDZLS2SVpWRA"
      },
      "source": [
        "import os\n",
        "import glob"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXUmQBdjjRSz"
      },
      "source": [
        "#cd /content/drive/MyDrive/datasets/"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A647jZGTIVfV"
      },
      "source": [
        "## 1- Data acquisition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM0BTYUsH0J7"
      },
      "source": [
        "###1.1- Get files from repository "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2XWYG1djOng"
      },
      "source": [
        "#!wget https://{bucket-name}.s3.eu-west-1.amazonaws.com/{file.zip}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ePlT65iHvE8"
      },
      "source": [
        "### 1.2 Extract zip file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY4poFTe_fiZ"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "#ZipFile(\"{file.zip}\").extractall('/content/drive/My Drive/datasets/')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Bife_AHmwn"
      },
      "source": [
        "### 1.3 Read all json files and insert into a csv file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9kYyZKupaKF"
      },
      "source": [
        "import os,json\n",
        "import pandas as pd\n",
        "\n",
        "# read json file\n",
        "def read_Json_And_Insert_Into_CSV(path_file_json, file_csv, root_path):\n",
        "  os.makedirs(root_path, exist_ok=True) # créer toute l'aborescence du fichier, crée le chemin\n",
        "  # read all json files\n",
        "  for file_name in [file for file in os.listdir(path_file_json) if file.endswith('.json')]:\n",
        "    with open(path_file_json + file_name) as json_file:\n",
        "      data = json.load(json_file)\n",
        "      df = pd.DataFrame.from_records(data)\n",
        "\n",
        "  # convert file to csv\n",
        "  df.to_csv(f'{root_path}/{file_csv}', sep=';')\n",
        "  return df # return du fichier csv"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIL79akW0R8f"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9HdEw7t0Sjx"
      },
      "source": [
        "###NB: ne pas utiliser les memes noms de variables à l'intérieur des fonctions et à l'extérieur"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCh5277VyUV4"
      },
      "source": [
        "path_json = '/content/drive/My Drive/datasets/nirra-log-bot/'\n",
        "root_csv = '/content/drive/My Drive/datasets/nirra-log-bot/csv'\n",
        "file_csv = 'file_name.csv'\n",
        "\n",
        "df_raw = read_Json_And_Insert_Into_CSV(path_json, file_csv, root_csv)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4q9nXruZEeS"
      },
      "source": [
        "df_raw.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L5hLbakIA9g"
      },
      "source": [
        "### Get total number of @ inside \"text\" column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1tAb3ZqNXxk",
        "outputId": "ad457f61-9b6b-4639-ca8b-f199188db27a"
      },
      "source": [
        " df_raw['text'].str.contains(\"@\").sum() # search total number of @ within text column"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "37"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h6pXkQwHaf_"
      },
      "source": [
        "### Test a slicing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOYCDMimN7Os"
      },
      "source": [
        "# slicing\n",
        "#df_raw[df_raw['text'].str.contains(\"@\")] # masque à l'intérieur des crochets"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "7TN5BDtoPudJ",
        "outputId": "0abed3b4-0915-41b3-a063-24a8617e93aa"
      },
      "source": [
        "df_raw[df_raw['text'] == None]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type</th>\n",
              "      <th>subtype</th>\n",
              "      <th>text</th>\n",
              "      <th>ts</th>\n",
              "      <th>bot_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [type, subtype, text, ts, bot_id]\n",
              "Index: []"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGScI9DACyVX"
      },
      "source": [
        "### Functions used in case of OKRA WEBHOOK, WALLET SUCCESS, SMS SUCCESS, SMS PAYLOAD Types request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CapeBj_Wqfkd"
      },
      "source": [
        "import regex\n",
        "import json\n",
        "\n",
        "#text_okra_webhook = df_raw['text'][0]\n",
        "#text_wallet = df_raw['text'][841]\n",
        "#sms_success = df_raw['text'][12]\n",
        "#sms_payload = df_raw['text'][11]\n",
        "def parse_wallet_sms_payload_success(text_type_request):\n",
        "  ''' la fonction permet de parser les types de requete \"Okra WebHook\", \"Wallet success\", \n",
        "      \"SMS Success\" et SMS Payload en object json.\n",
        "      Elle prend en paramètre le text contenu dans le type de requete,\n",
        "      elle retourne un objet de type JSON.'''\n",
        "\n",
        "  pattern = regex.compile(r'\\{(?:[^{}]|(?R))*}')\n",
        "  resul_patt = pattern.findall(text_type_request)\n",
        "  res = resul_patt[0].replace(\"\\\\\", \" \")\n",
        "  s = json.loads(res)\n",
        "  out_dict = {} # dictionnary vide\n",
        "  for key, value in s.items():\n",
        "    out_dict[key.strip()] = value # à la clé on passe chaque valeur, strip() enlève les espaces au début et à la fin.\n",
        "\n",
        "\n",
        "  out_dump = json.dumps(out_dict) # input est un dictionnaire et ça retourne un json sous forme string\n",
        "  out_wallet_success = json.loads(out_dump) # convertir le string json en object json.\n",
        "  return out_wallet_success\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f_3vBokI4W_"
      },
      "source": [
        "# Test Okra WebHook\n",
        "okra = parse_wallet_sms_payload_success(df_raw['text'][0])\n",
        "okra\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r42HQDd7IqD4"
      },
      "source": [
        "# Test Wallet Success\n",
        "wallet_success = parse_wallet_sms_payload_success(df_raw['text'][841])\n",
        "print(wallet_success.get('account_name'))\n",
        "print(wallet_success.get('account_number'))\n",
        "print(wallet_success.get('bvn'))\n",
        "print(wallet_success.get('requestSuccessful'))\n",
        "print(wallet_success.get('responseCode'))\n",
        "print(wallet_success.get('responseMessage'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vol_EjbkIdeu",
        "outputId": "c50be946-0325-4810-a4a6-7ecadac9979c"
      },
      "source": [
        "# Test SMS Sucess\n",
        "sms_succ = parse_wallet_sms_payload_success(df_raw['text'][12])\n",
        "sms_succ.get('response').get('cost ')\n",
        "sms_succ.get('response').get('status ')\n",
        "sms_succ.get('response').get('totalsent ')\n",
        "sms_succ.get('response')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cost ': 2, 'status ': 'SUCCESS ', 'totalsent ': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIJQYyhvJfry"
      },
      "source": [
        "# Test SMS Payload\n",
        "sms_payload = parse_wallet_sms_payload_success(df_raw['text'][11])\n",
        "print(sms_payload)\n",
        "print(sms_payload.get('message'))\n",
        "print(sms_payload.get('phone'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxLQChIkYxVc"
      },
      "source": [
        "# Function to handle row with type request \"LEADWAY SUCCESS\" and concatenate rows\n",
        "### NB: faire un docstring (''' ''') pour chaque fonction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4c-G8ZBYp_Z"
      },
      "source": [
        "import re\n",
        "\n",
        "# la fonction doit prendre en paramètre quelque chose\n",
        "def parse_and_concatenate_Leadway_Success_Rows(df_raw):\n",
        "  '''Cette fonction permet de parser et de concatener le texte qui a LEADWAY SUCCESS\n",
        "     comme type de requete\n",
        "     elle prend comme paramètre un dataframe et retourne les valeurs suivantes:\n",
        "     - un texte concatené\n",
        "     - l'index de la 1ère ligne qu'on va utiliser ensuite pour l'effacer\n",
        "     - l'index de la dernière ligne qu'on va utiliser ensuite pour l'effacer '''\n",
        "\n",
        "  first_index = 0\n",
        "  last_index = 0\n",
        "  text_leadway_concat = ''\n",
        "  for index, row in df_raw.iterrows():  # boucler sur les colonnes de type text\n",
        "      text_row = row['text']  \n",
        "      if re.search('LEADWAY SUCCESS', text_row):\n",
        "        text_leadway_concat = text_row\n",
        "        first_index = index\n",
        "        first_index +=1\n",
        "        new_df = df_raw[first_index:]\n",
        "        for first_index, new_row in new_df.iterrows():\n",
        "          xxx = new_row['text']      \n",
        "          if not xxx.startswith('['):          \n",
        "            first_index += 1\n",
        "            text_leadway_concat = text_row + xxx       \n",
        "          elif xxx.startswith('['):\n",
        "            last_index = first_index-1\n",
        "            break\n",
        "\n",
        "\n",
        "  return text_leadway_concat, first_index, last_index\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFfztKeA8XgQ"
      },
      "source": [
        "leadway_succ, first_index_succ, last_index_succ = parse_and_concatenate_Leadway_Success_Rows(df_raw)\n",
        "leadway_succ, first_index_succ, last_index_succ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mliPyobDr4k"
      },
      "source": [
        "### Function to parse row Leadway Success to json"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2FJ1cmnDl0A"
      },
      "source": [
        "# use this function when type request is LEADWAY SUCCESS\n",
        "import regex\n",
        "import json\n",
        "\n",
        "\n",
        "def parse_Leadway_Success_Row(text_leadway):\n",
        "  ''' fonction permettant de parser le text concatené pour le type de requet LEADWAY SUCCESS\n",
        "      elle retourner un dictionnaire.'''\n",
        "  pattern = regex.compile(r'\\{(?:[^{}]|(?R))*}')\n",
        "  resul_patt = pattern.findall(text_leadway)\n",
        "  resul_patt[0] = resul_patt[0].replace(\"\\\\\", \"\")\n",
        "  x = resul_patt[0].replace(\"make,\", \"\")\n",
        "  y = x.replace('\"\"makeName\"', '\"makeName\"')\n",
        "  z = json.loads(y)\n",
        "  vehicleMake = z.get('vehicleMake')\n",
        "  leadway_dict = {}\n",
        "  for element in vehicleMake:\n",
        "    leadway_dict[element['id']] = element['makeName']\n",
        "\n",
        "  return leadway_dict\n",
        "\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyu1DwSqdRrV"
      },
      "source": [
        "print(leadway_succ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-K_HcgpL0Iv"
      },
      "source": [
        "# TEST LEADWAY SUCCESS\n",
        "resultat_leadway = parse_Leadway_Success_Row(leadway_succ)\n",
        "\n",
        "for i in resultat_leadway:\n",
        "  print(resultat_leadway.get(i))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZfl792oBMyw",
        "outputId": "7cd61d9e-6614-4a32-a129-39e03c98ec01"
      },
      "source": [
        "2356 in df_raw.index"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho5dPdBwml27"
      },
      "source": [
        "### Function to handle log level \"Error\"\n",
        "NB: faire un docstring (''' ''') pour chaque fonction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLMS5PrDoPIV"
      },
      "source": [
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_Error_Row(error_row):\n",
        "  error_row = error_row.replace('\"', \"'\")\n",
        "  pattern = regex.compile(r\"{?[a-z :A-Z 0-9\\\\,=_`']+selfie\")\n",
        "  resul_patt = pattern.findall(error_row)\n",
        "  res = resul_patt[0].replace(\"\\\\\", \" \")\n",
        "  res = res.replace(\"'name'\", \"name\").replace(\"`\", \"\").replace(\"'18'\", \"18\").replace(\"'monthly'\", \"monthly\")\n",
        "  res = res+'\"}'\n",
        "  res = res.replace(\"'\", '\"')\n",
        "  s = json.loads(res)\n",
        "  out_error_dict = {} # dictionnary vide\n",
        "  for key, value in s.items():\n",
        "    out_error_dict[key.strip()] = value # à la clé on passe chaque valeur, strip() enlève les espaces au début et à la fin.\n",
        "\n",
        "  out_error_dump = json.dumps(out_error_dict) # input est un dictionnaire et ça retourne un json sous forme string\n",
        "  out_error_text = json.loads(out_error_dump) # convertir le string json en object json.\n",
        "  return out_error_text\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RZvPvv3FgyK"
      },
      "source": [
        "error_text = parse_Error_Row(df_raw['text'][2355])\n",
        "print(error_text)\n",
        "print(error_text.get('code'))\n",
        "print(error_text.get('errno'))\n",
        "print(error_text.get('sqlMessage'))\n",
        "print(error_text.get('sqlState'))\n",
        "print(error_text.get('index'))\n",
        "print(error_text.get('sql'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY9EAlNYIv5x"
      },
      "source": [
        "### Function to convert data to timestamp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd-prG9NMlJz"
      },
      "source": [
        "import datetime\n",
        "\n",
        "# function to convert date to Timestamp\n",
        "def convertToTimestamp(str):\n",
        "  element = datetime.datetime.strptime(str,\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "  return datetime.datetime.timestamp(element)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-rZUfpUM3lc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f5a30e5-8a36-476e-e8be-fb0d86722ef5"
      },
      "source": [
        "timestamp = convertToTimestamp('2021-09-05T07:03:55.223Z')\n",
        "timestamp"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1630825435.223"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8TrJS3NHJ6x"
      },
      "source": [
        "## 2 Data model\n",
        "### 2.1 Parse rows of dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "585dRkFQPUbB"
      },
      "source": [
        "### 2.1.1 créer un dictionnary pour les regex."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88lZf2qqvQag"
      },
      "source": [
        "type_request_dictionnary = {}\n",
        "regex_list_api_request = []\n",
        "regex_list_api_request.append('[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
        "regex_list_api_request.append('/[/a-z 0-9?=&;/_A-Z+]+')\n",
        "regex_list_api_request.append('(\\d{4})-(\\d\\d)-(\\d\\d)T(\\d\\d):(\\d\\d):(\\d\\d).(\\d{3})*[a-zA-Z]')\n",
        "regex_list_api_request.append('[0-9]+')\n",
        "\n",
        "type_request_dictionnary['API REQUEST'] = regex_list_api_request\n",
        "\n",
        "regex_list_client_mobile = []\n",
        "regex_list_client_mobile.append('[\\w.+-]+@[\\w-]+\\.[\\w.-]+')\n",
        "regex_list_client_mobile.append('(\\d{4})-(\\d\\d)-(\\d\\d)T(\\d\\d):(\\d\\d):(\\d\\d).(\\d{3})*[a-zA-Z]')\n",
        "regex_list_client_mobile.append('[0-9]+')\n",
        "\n",
        "type_request_dictionnary['CLIENT MOBILE LOGIN'] = regex_list_client_mobile\n",
        "\n",
        "type_request_dictionnary['SMS PAYLOAD'] = '\\{(?:[^{}]|(?R))*}'\n",
        "type_request_dictionnary['SMS SUCCESS'] = '\\{(?:[^{}]|(?R))*}'\n",
        "type_request_dictionnary['WALLET SUCCESS'] = '\\{(?:[^{}]|(?R))*}'\n",
        "type_request_dictionnary['LEADWAY SUCCESS'] = '\\{(?:[^{}]|(?R))*}'"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceqzAG_lPsE7"
      },
      "source": [
        "### 2.1.2 Functions. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AJ6V9HulbOq"
      },
      "source": [
        "### Handle DataFrame Error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN-IUROmk7Bh"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_error(df_):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_Col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "  error_code_col = []\n",
        "  error_number_col = []\n",
        "  error_sql_message_col = []\n",
        "  error_sql_state_col = []\n",
        "  error_index_col = []\n",
        "  error_sql_col = []\n",
        "\n",
        "  \n",
        "  list_column_none_level_log_error = []\n",
        "  list_column_none = [message_sms_payload_col, totalsent_col, cost_col, status_col, email_col,\n",
        "                      phone_Col, endpoint_Col, date_col, bvn_col, requestSuccessful_col, responseMessage_col,\n",
        "                      responseCode_col, account_name_col, account_number_col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_Col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col,\n",
        "                    error_code_col, error_number_col, error_sql_message_col, error_sql_state_col, \n",
        "                    error_index_col, error_sql_col]\n",
        "\n",
        "  for index, row in df_.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "    \n",
        "    if re.search('error', str_text):\n",
        "        log_level = re.search('error', str_text)\n",
        "          \n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)\n",
        "\n",
        "        if re.search('LOAN ERROR', str_text):\n",
        "            type_of_request = re.search('LOAN ERROR', str_text)\n",
        "            loan_error = parse_Error_Row(str_text) \n",
        "            try:\n",
        "              error_code_col.append(loan_error.get('code'))\n",
        "              print(error_code_col)\n",
        "            except AttributeError:\n",
        "              error_code_col.append(None)\n",
        "            try:\n",
        "              print(loan_error.get('errno'))\n",
        "              error_number_col.append(loan_error.get('errno'))\n",
        "              print('error number :')\n",
        "              print(error_number_col)\n",
        "            except AttributeError:\n",
        "              error_number_col.append(None)\n",
        "            try:\n",
        "              error_sql_message_col.append(loan_error.get('sqlMessage'))\n",
        "            except AttributeError:\n",
        "              error_sql_message_col.append(None)\n",
        "            try:\n",
        "              error_sql_state_col.append(loan_error.get('sqlState'))\n",
        "            except AttributeError:\n",
        "              error_sql_state_col.append(None)\n",
        "            try:\n",
        "              error_index_col.append(loan_error.get('index'))\n",
        "            except AttributeError:\n",
        "              error_index_col.append(None)\n",
        "            try:\n",
        "              error_sql_col.append(loan_error.get('sql'))\n",
        "            except AttributeError:\n",
        "              error_sql_col.append(None)       \n",
        "\n",
        "            try:\n",
        "                type_request_col.append(type_of_request.group(0))\n",
        "            except AttributeError:\n",
        "                type_request_col.append(None)\n",
        "\n",
        "            for p in range(len(list_column_none)):\n",
        "              list_column_none[p].append(None)\n",
        "  \n",
        "  \n",
        "  #for x in range(len(list_all_colum)):\n",
        "   # print(len(list_all_colum[x]))\n",
        "  \n",
        "  df_['Type_Request'] = type_request_col\n",
        "  df_['Phone_Number'] = phone_Col\n",
        "  df_['Date'] = date_col\n",
        "  df_['EndPoint'] = endpoint_Col\n",
        "  df_['Log_Level'] = log_level_col\n",
        "  df_['Email'] = email_col\n",
        "  df_['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_['Total Sent'] = totalsent_col\n",
        "  df_['Cost'] = cost_col\n",
        "  df_['Status'] = status_col\n",
        "  df_['Account Number'] = account_number_col\n",
        "  df_['Account Name'] = account_name_col\n",
        "  df_['BVN'] = bvn_col\n",
        "  df_['Request Successful'] = requestSuccessful_col\n",
        "  df_['Response Message'] = responseMessage_col\n",
        "  df_['Response Code'] = responseCode_col\n",
        "  df_['Error Code'] = error_code_col\n",
        "  df_['Error Number'] = error_number_col\n",
        "  df_['Error Sql Message'] = error_sql_message_col\n",
        "  df_['Error Sql State'] = error_sql_state_col\n",
        "  df_['Error Index'] = error_index_col\n",
        "  df_['Error Sql'] = error_sql_col\n",
        "\n",
        "\n",
        "  return df_"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV-o576_eO32"
      },
      "source": [
        "### Handle DataFrame for error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3poKQQZa6cA0"
      },
      "source": [
        "df_error = df_raw[df_raw['text'].str.contains('LOAN ERROR')]\n",
        "resutat_error = parse_row_error(df_error)\n",
        "resutat_error.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK5QuIgk9xxo"
      },
      "source": [
        "#df_raw = df_raw.drop(2355)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u--eSd8pBL03"
      },
      "source": [
        "df_raw.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B2AIKGMEuuG"
      },
      "source": [
        "### Handle DataFrame for API REQUEST Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0d5QsJ6Db5s"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_api_request(df_api_request):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_Col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "\n",
        "  list_column_none_api_request = []\n",
        "  list_column_none_api_request = [message_sms_payload_col, totalsent_col, cost_col, status_col,\n",
        "                                  bvn_col, requestSuccessful_col, responseMessage_col,\n",
        "                                  responseCode_col, account_name_col, account_number_col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_Col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "  for index, row in df_api_request.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "\n",
        "    # check if the row contains \"info\" string\n",
        "    if re.search('info', str_text):\n",
        "        log_level = re.search('info', str_text)\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)        \n",
        "        # check if the row contains an email address \n",
        "        # pour tous les types request créer un dictionnaire dans lequel mapper\n",
        "        # key = type de request et value = les regex définis\n",
        "        # pour chaque condition IF créer une liste de colonnes auxquelles affecter None\n",
        "        if 'mailto' in str_text:\n",
        "            if re.search('API REQUEST', str_text):\n",
        "                type_of_request = re.search('API REQUEST', str_text)                \n",
        "                phone_or_email_api_req = re.search(type_request_dictionnary['API REQUEST'][0], str_text)                              \n",
        "                endpoint = re.search(type_request_dictionnary['API REQUEST'][1], str_text)\n",
        "                pattern = type_request_dictionnary['API REQUEST'][2]\n",
        "                datepattern = re.compile(\"(?:%s)\"%(pattern))\n",
        "                datematcher = datepattern.search(str_text)  # extract date\n",
        "\n",
        "                for i in range(len(list_column_none_api_request)):\n",
        "                  list_column_none_api_request[i].append(None)\n",
        "                               \n",
        "                try:\n",
        "                  type_request_col.append(type_of_request.group(0)) # add type request inside type request column\n",
        "                except AttributeError:\n",
        "                  type_request_col.append(None)               \n",
        "                try:\n",
        "                  email_col.append(phone_or_email_api_req.group(0)) # add email inside email column\n",
        "                  phone_Col.append(None)  # in this case there is no phone number\n",
        "                except AttributeError:\n",
        "                  email_col.append(None)\n",
        "                try:\n",
        "                  endpoint_Col.append(endpoint.group(0)) # add endpoint inside endpoint column\n",
        "                except AttributeError:\n",
        "                  endpoint_Col.append(None)\n",
        "                try:\n",
        "                  date_col.append(convertToTimestamp(datematcher.group(0))) # convert date to timestamp and add it inside date column\n",
        "                except AttributeError:\n",
        "                  date_col.append(None)\n",
        "              \n",
        "\n",
        "        elif 'mailto' not in str_text:\n",
        "            if re.search('API REQUEST', str_text):\n",
        "                type_of_request = re.search('API REQUEST', str_text)                            \n",
        "                # extract a phone number for API REQUEST\n",
        "                phone_or_email_api_req = re.search(type_request_dictionnary['API REQUEST'][3], str_text)                              \n",
        "                endpoint = re.search(type_request_dictionnary['API REQUEST'][1], str_text)\n",
        "                pattern = type_request_dictionnary['API REQUEST'][2]\n",
        "                datepattern = re.compile(\"(?:%s)\"%(pattern))\n",
        "                datematcher = datepattern.search(str_text)  # extract date\n",
        "\n",
        "                for i in range(len(list_column_none_api_request)):\n",
        "                  list_column_none_api_request[i].append(None)\n",
        "\n",
        "                try:\n",
        "                  phone_Col.append(phone_or_email_api_req.group(0)) # add phone number inside phone number column\n",
        "                  email_col.append(None) # in this case there is no email address\n",
        "                except AttributeError:\n",
        "                  phone_Col.append(None)\n",
        "                try:\n",
        "                  type_request_col.append(type_of_request.group(0))\n",
        "                except AttributeError:\n",
        "                  type_request_col.append(None)\n",
        "                try:\n",
        "                  endpoint_Col.append(endpoint.group(0)) # add endpoint inside endpoint column\n",
        "                except AttributeError:\n",
        "                  endpoint_Col.append(None)\n",
        "                try:\n",
        "                  date_col.append(convertToTimestamp(datematcher.group(0))) # convert date to timestamp and add it inside date column\n",
        "                except AttributeError:\n",
        "                  date_col.append(None)\n",
        "\n",
        "  df_api_request['Type_Request'] = type_request_col\n",
        "  df_api_request['Phone_Number'] = phone_Col\n",
        "  df_api_request['Date'] = date_col\n",
        "  df_api_request['EndPoint'] = endpoint_Col\n",
        "  df_api_request['Log_Level'] = log_level_col\n",
        "  df_api_request['Email'] = email_col\n",
        "  df_api_request['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_api_request['Total Sent'] = totalsent_col\n",
        "  df_api_request['Cost'] = cost_col\n",
        "  df_api_request['Status'] = status_col\n",
        "  df_api_request['Account Number'] = account_number_col\n",
        "  df_api_request['Account Name'] = account_name_col\n",
        "  df_api_request['BVN'] = bvn_col\n",
        "  df_api_request['Request Successful'] = requestSuccessful_col\n",
        "  df_api_request['Response Message'] = responseMessage_col\n",
        "  df_api_request['Response Code'] = responseCode_col\n",
        " \n",
        "  return df_api_request"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfHNYGxLEpl9"
      },
      "source": [
        "df_api_request_ = df_raw[df_raw['text'].str.contains('API REQUEST')]\n",
        "resutat_df_api = parse_row_api_request(df_api_request_)\n",
        "resutat_df_api.head(10)\n",
        "resutat_df_api.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBE4JmynFp2_"
      },
      "source": [
        "### Handle DataFrame for Client Mobile Login Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnVrzUqSFpV1"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_client_mobile_login(df_client_mob):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_Col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "\n",
        "  list_column_none_client_mobile = []\n",
        "  list_column_none_client_mobile = [message_sms_payload_col, totalsent_col, cost_col, status_col,\n",
        "                                  account_number_col, bvn_col, requestSuccessful_col, responseMessage_col,\n",
        "                                  responseCode_col, account_name_col, endpoint_Col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_Col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "  for index, row in df_client_mob.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "\n",
        "    # check if the row contains \"info\" string\n",
        "    if re.search('info', str_text):\n",
        "        log_level = re.search('info', str_text)\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)        \n",
        "        # check if the row contains an email address \n",
        "        # pour tous les types request créer un dictionnaire dans lequel mapper\n",
        "        # key = type de request et value = les regex définis\n",
        "        # pour chaque condition IF créer une liste de colonnes auxquelles affecter None\n",
        "        if 'mailto' in str_text:\n",
        "            if re.search('CLIENT MOBILE LOGIN', str_text):   # CLIENT MOBILE LOGIN with email address\n",
        "                  type_of_request = re.search('CLIENT MOBILE LOGIN', str_text)\n",
        "                  # extract address email for CLIENT MOBILE LOGIN\n",
        "                  phone_or_email_client_mobile = re.search(type_request_dictionnary['CLIENT MOBILE LOGIN'][0], str_text)                               \n",
        "                  pattern = type_request_dictionnary['CLIENT MOBILE LOGIN'][1]\n",
        "                  datepattern = re.compile(\"(?:%s)\"%(pattern))\n",
        "                  datematcher = datepattern.search(str_text)  # extract date for CLIENT MOBILE LOGIN type request\n",
        "                  \n",
        "                  for j in range(len(list_column_none_client_mobile)):\n",
        "                    list_column_none_client_mobile[j].append(None)\n",
        "\n",
        "                  try:\n",
        "                    type_request_col.append(type_of_request.group(0)) # add type request inside type request column\n",
        "                  except AttributeError:\n",
        "                    type_request_col.append(None) \n",
        "                  try:\n",
        "                    email_col.append(phone_or_email_client_mobile.group(0)) # add email inside email column\n",
        "                    phone_Col.append(None)  # in this case there is no phone number\n",
        "                  except AttributeError:\n",
        "                    email_col.append(None)\n",
        "                  try:\n",
        "                    date_col.append(convertToTimestamp(datematcher.group(0))) # convert date to timestamp and add it inside date column\n",
        "                  except AttributeError:\n",
        "                    date_col.append(None)\n",
        "\n",
        "        elif 'mailto' not in str_text:\n",
        "            if re.search('CLIENT MOBILE LOGIN', str_text): # when type request is CLIENT MOBILE LOGIN, there is no EndPoint\n",
        "                type_of_request = re.search('CLIENT MOBILE LOGIN', str_text)\n",
        "                # extract a phone number for CLIENT MOBILE LOGIN\n",
        "                phone_or_email_client_mobile = re.search(type_request_dictionnary['CLIENT MOBILE LOGIN'][2], str_text)                  \n",
        "                pattern = type_request_dictionnary['CLIENT MOBILE LOGIN'][1]\n",
        "                datepattern = re.compile(\"(?:%s)\"%(pattern))\n",
        "                datematcher = datepattern.search(str_text)  # extract date\n",
        "\n",
        "                for j in range(len(list_column_none_client_mobile)):\n",
        "                    list_column_none_client_mobile[j].append(None)\n",
        "\n",
        "                try:\n",
        "                  phone_Col.append(phone_or_email_client_mobile.group(0))\n",
        "                  email_col.append(None)\n",
        "                except AttributeError:\n",
        "                  phone_Col.append(None)\n",
        "                try:\n",
        "                  type_request_col.append(type_of_request.group(0))\n",
        "                except AttributeError:\n",
        "                  type_request_col.append(None)\n",
        "                try:\n",
        "                  date_col.append(convertToTimestamp(datematcher.group(0))) # convert date to timestamp and add it inside date column\n",
        "                except AttributeError:\n",
        "                  date_col.append(None) \n",
        "\n",
        "  df_client_mob['Type_Request'] = type_request_col\n",
        "  df_client_mob['Phone_Number'] = phone_Col\n",
        "  df_client_mob['Date'] = date_col\n",
        "  df_client_mob['EndPoint'] = endpoint_Col\n",
        "  df_client_mob['Log_Level'] = log_level_col\n",
        "  df_client_mob['Email'] = email_col\n",
        "  df_client_mob['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_client_mob['Total Sent'] = totalsent_col\n",
        "  df_client_mob['Cost'] = cost_col\n",
        "  df_client_mob['Status'] = status_col\n",
        "  df_client_mob['Account Number'] = account_number_col\n",
        "  df_client_mob['Account Name'] = account_name_col\n",
        "  df_client_mob['BVN'] = bvn_col\n",
        "  df_client_mob['Request Successful'] = requestSuccessful_col\n",
        "  df_client_mob['Response Message'] = responseMessage_col\n",
        "  df_client_mob['Response Code'] = responseCode_col\n",
        " \n",
        "  return df_client_mob"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTlHywpVGsw4"
      },
      "source": [
        "df_client_mobile_login = df_raw[df_raw['text'].str.contains('CLIENT MOBILE LOGIN')]\n",
        "resutat_df_client_mobile_login = parse_row_client_mobile_login(df_client_mobile_login)\n",
        "resutat_df_client_mobile_login.head()\n",
        "resutat_df_client_mobile_login.info()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm5iY2LGHekm"
      },
      "source": [
        "### Handle DataFrame for SMS PAYLOAD Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2Iqpe8gHore"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_sms_payload_function(df_sms_payload):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_Col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "  \n",
        "  list_column_none_sms_payload = []\n",
        "  list_column_none_sms_payload = [totalsent_col, cost_col, status_col,\n",
        "                                  account_number_col, bvn_col, requestSuccessful_col, responseMessage_col,\n",
        "                                  responseCode_col, account_name_col, email_col, endpoint_Col, date_col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_Col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "  for index, row in df_sms_payload.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "\n",
        "    # check if the row contains \"info\" string\n",
        "    if re.search('info', str_text):\n",
        "        log_level = re.search('info', str_text)\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)             \n",
        "        if 'mailto' not in str_text:\n",
        "            if re.search('SMS PAYLOAD', str_text):\n",
        "                type_of_request = re.search('SMS PAYLOAD', str_text)            \n",
        "                sms_payload = parse_wallet_sms_payload_success(str_text)               \n",
        "                for l in range(len(list_column_none_sms_payload)):\n",
        "                    list_column_none_sms_payload[l].append(None)             \n",
        "                try:\n",
        "                  type_request_col.append(type_of_request.group(0))\n",
        "                except AttributeError:\n",
        "                  type_request_col.append(None)\n",
        "                try:\n",
        "                  phone_Col.append(sms_payload.get('phone'))\n",
        "                except AttributeError:\n",
        "                  phone_Col.append(None)\n",
        "                try:\n",
        "                  message_sms_payload_col.append(sms_payload.get('message'))\n",
        "                except AttributeError:\n",
        "                  message_sms_payload_col.append(None)\n",
        "                     \n",
        "        elif re.search('OKRA PAYLOAD', str_text): # Nothing\n",
        "          type_of_request = re.search('OKRA PAYLOAD', str_text)\n",
        "        elif re.search('OKRA SUCCESS', str_text):   # Nothing\n",
        "          type_of_request = re.search('OKRA SUCCESS', str_text)\n",
        "        elif re.search('VTPASS SUCCESS', str_text):   # Nothing\n",
        "          type_of_request = re.search('VTPASS SUCCESS', str_text)  \n",
        "\n",
        "  df_sms_payload['Type_Request'] = type_request_col\n",
        "  df_sms_payload['Phone_Number'] = phone_Col\n",
        "  df_sms_payload['Date'] = date_col\n",
        "  df_sms_payload['EndPoint'] = endpoint_Col\n",
        "  df_sms_payload['Log_Level'] = log_level_col\n",
        "  df_sms_payload['Email'] = email_col\n",
        "  df_sms_payload['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_sms_payload['Total Sent'] = totalsent_col\n",
        "  df_sms_payload['Cost'] = cost_col\n",
        "  df_sms_payload['Status'] = status_col\n",
        "  df_sms_payload['Account Number'] = account_number_col\n",
        "  df_sms_payload['Account Name'] = account_name_col\n",
        "  df_sms_payload['BVN'] = bvn_col\n",
        "  df_sms_payload['Request Successful'] = requestSuccessful_col\n",
        "  df_sms_payload['Response Message'] = responseMessage_col\n",
        "  df_sms_payload['Response Code'] = responseCode_col\n",
        " \n",
        "  return df_sms_payload\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7etb-rPaJHMZ"
      },
      "source": [
        "df_sms_payload_ = df_raw[df_raw['text'].str.contains('SMS PAYLOAD')]\n",
        "resutat_df_sms_payload = parse_row_sms_payload_function(df_sms_payload_)\n",
        "resutat_df_sms_payload.head()\n",
        "#resutat_df_sms_payload.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz-7pflyJ_Fs"
      },
      "source": [
        "### Handle DataFrame for SMS Success Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJG8_sMYKDKw"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_sms_success_function(df_sms_success):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_Col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "  \n",
        "  list_column_none_sms_success = []\n",
        "  list_column_none_sms_success = [message_sms_payload_col, account_number_col, bvn_col, requestSuccessful_col, \n",
        "                                  responseMessage_col, responseCode_col, account_name_col, email_col, \n",
        "                                  phone_Col, endpoint_Col, date_col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_Col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "  for index, row in df_sms_success.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "\n",
        "    # check if the row contains \"info\" string\n",
        "    if re.search('info', str_text):\n",
        "        log_level = re.search('info', str_text)\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)             \n",
        "        if 'mailto' not in str_text:\n",
        "            if re.search('SMS SUCCESS', str_text): \n",
        "                type_of_request = re.search('SMS SUCCESS', str_text)\n",
        "                sms_success = parse_wallet_sms_payload_success(str_text)                \n",
        "                for m in range(len(list_column_none_sms_success)):\n",
        "                    list_column_none_sms_success[m].append(None) \n",
        "           \n",
        "                try:\n",
        "                  type_request_col.append(type_of_request.group(0))\n",
        "                except AttributeError:\n",
        "                  type_request_col.append(None)\n",
        "                try:                 \n",
        "                  totalsent_col.append(sms_success.get('response').get('totalsent '))\n",
        "                except AttributeError:\n",
        "                  totalsent_col.append(None)\n",
        "                try:                 \n",
        "                  cost_col.append(sms_success.get('response').get('cost '))\n",
        "                except AttributeError:\n",
        "                  cost_col.append(None)\n",
        "                try:                 \n",
        "                  status_col.append(sms_success.get('response').get('status '))\n",
        "                except AttributeError:\n",
        "                  status_col.append(None)\n",
        "                     \n",
        "        elif re.search('OKRA PAYLOAD', str_text): # Nothing\n",
        "          type_of_request = re.search('OKRA PAYLOAD', str_text)\n",
        "        elif re.search('OKRA SUCCESS', str_text):   # Nothing\n",
        "          type_of_request = re.search('OKRA SUCCESS', str_text)\n",
        "        elif re.search('VTPASS SUCCESS', str_text):   # Nothing\n",
        "          type_of_request = re.search('VTPASS SUCCESS', str_text)    \n",
        "\n",
        "  df_sms_success['Type_Request'] = type_request_col\n",
        "  df_sms_success['Phone_Number'] = phone_Col\n",
        "  df_sms_success['Date'] = date_col\n",
        "  df_sms_success['EndPoint'] = endpoint_Col\n",
        "  df_sms_success['Log_Level'] = log_level_col\n",
        "  df_sms_success['Email'] = email_col\n",
        "  df_sms_success['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_sms_success['Total Sent'] = totalsent_col\n",
        "  df_sms_success['Cost'] = cost_col\n",
        "  df_sms_success['Status'] = status_col\n",
        "  df_sms_success['Account Number'] = account_number_col\n",
        "  df_sms_success['Account Name'] = account_name_col\n",
        "  df_sms_success['BVN'] = bvn_col\n",
        "  df_sms_success['Request Successful'] = requestSuccessful_col\n",
        "  df_sms_success['Response Message'] = responseMessage_col\n",
        "  df_sms_success['Response Code'] = responseCode_col\n",
        " \n",
        "  return df_sms_success\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFWOAd_5KmGM"
      },
      "source": [
        "df_sms_success_ = df_raw[df_raw['text'].str.contains('SMS SUCCESS')]\n",
        "resutat_df_sms_success = parse_row_sms_success_function(df_sms_success_)\n",
        "resutat_df_sms_success.head()\n",
        "#resutat_df_sms_success.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMxkpxxcK72K"
      },
      "source": [
        "### Handle DataFrame for WALLET SUCCESS Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-XB7evJLGvD"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_wallet_success_function(df_wallet_success):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_Col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "\n",
        "  list_column_none_wallet_success = []\n",
        "  list_column_none_wallet_success = [totalsent_col, message_sms_payload_col, cost_col, status_col, \n",
        "                                     email_col, phone_Col, endpoint_Col, date_col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_Col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "  \n",
        "  for index, row in df_wallet_success.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "\n",
        "    # check if the row contains \"info\" string\n",
        "    if re.search('info', str_text):\n",
        "        log_level = re.search('info', str_text)\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)             \n",
        "        if 'mailto' not in str_text:\n",
        "            if re.search('WALLET SUCCESS', str_text):\n",
        "                  wallet_success = parse_wallet_sms_payload_success(str_text)\n",
        "                  type_of_request = re.search('WALLET SUCCESS', str_text)\n",
        "\n",
        "                  try:\n",
        "                    type_request_col.append(type_of_request.group(0))\n",
        "                  except AttributeError:\n",
        "                    type_request_col.append(None)\n",
        "                  try:\n",
        "                    account_number_col.append(wallet_success.get('account_number'))\n",
        "                  except AttributeError:\n",
        "                    account_number_col.append(None)\n",
        "                  try:\n",
        "                    account_name_col.append(wallet_success.get('account_name'))\n",
        "                  except AttributeError:\n",
        "                    account_name_col.append(None)\n",
        "                  try:\n",
        "                    bvn_col.append(wallet_success.get('bvn'))\n",
        "                  except AttributeError:\n",
        "                    bvn_col.append(None)\n",
        "                  try:\n",
        "                    requestSuccessful_col.append(wallet_success.get('requestSuccessful'))\n",
        "                  except AttributeError:\n",
        "                    requestSuccessful_col.append(None)\n",
        "                  try:\n",
        "                    responseMessage_col.append((wallet_success.get('responseMessage')))\n",
        "                  except AttributeError:\n",
        "                    responseMessage_col.append(None)\n",
        "                  try:\n",
        "                    responseCode_col.append((wallet_success.get('responseCode')))\n",
        "                  except AttributeError:\n",
        "                    responseCode_col.append(None)\n",
        "                  for n in range(len(list_column_none_wallet_success)):\n",
        "                    list_column_none_wallet_success[n].append(None) \n",
        "\n",
        "                     \n",
        "        elif re.search('OKRA PAYLOAD', str_text): # Nothing\n",
        "          type_of_request = re.search('OKRA PAYLOAD', str_text)\n",
        "        elif re.search('OKRA SUCCESS', str_text):   # Nothing\n",
        "          type_of_request = re.search('OKRA SUCCESS', str_text)\n",
        "        elif re.search('VTPASS SUCCESS', str_text):   # Nothing\n",
        "          type_of_request = re.search('VTPASS SUCCESS', str_text)    \n",
        "\n",
        "  df_wallet_success['Type_Request'] = type_request_col\n",
        "  df_wallet_success['Phone_Number'] = phone_Col\n",
        "  df_wallet_success['Date'] = date_col\n",
        "  df_wallet_success['EndPoint'] = endpoint_Col\n",
        "  df_wallet_success['Log_Level'] = log_level_col\n",
        "  df_wallet_success['Email'] = email_col\n",
        "  df_wallet_success['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_wallet_success['Total Sent'] = totalsent_col\n",
        "  df_wallet_success['Cost'] = cost_col\n",
        "  df_wallet_success['Status'] = status_col\n",
        "  df_wallet_success['Account Number'] = account_number_col\n",
        "  df_wallet_success['Account Name'] = account_name_col\n",
        "  df_wallet_success['BVN'] = bvn_col\n",
        "  df_wallet_success['Request Successful'] = requestSuccessful_col\n",
        "  df_wallet_success['Response Message'] = responseMessage_col\n",
        "  df_wallet_success['Response Code'] = responseCode_col\n",
        " \n",
        "  return df_wallet_success\n",
        "             "
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVPZi-uGMbDU"
      },
      "source": [
        "df_wallet_success_ = df_raw[df_raw['text'].str.contains('WALLET SUCCESS')]\n",
        "resutat_df_wallet_success = parse_row_wallet_success_function(df_wallet_success_)\n",
        "resutat_df_wallet_success.head()\n",
        "#resutat_df_wallet_success.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4IzATOC-s2M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53f880b4-59e1-4317-b2bf-fc479684a706"
      },
      "source": [
        "okra = parse_wallet_sms_payload_success(df_raw['text'][0])\n",
        "print(okra.get('authorization').get('customer '))\n",
        "print(okra.get('authorization').get('env '))\n",
        "print(okra.get('authorization').get('owner '))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5f879cdbaa47f26f4750f645 \n",
            "production \n",
            "6107cf082726121b90b047f8 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mps5LhixM8Zo"
      },
      "source": [
        "### Handle DataFrame for OKRA WEBHOOK Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upUDyIGEM8yc"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_okra_webhook_function(df_okra):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_Col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "  accountId_col = []\n",
        "  authorization_v_col = []\n",
        "  authorization_id_col = []\n",
        "  authorization_customer_col = []\n",
        "  authorization_account_col = []\n",
        "  authorization_account_id_col = []\n",
        "  authorization_account_manual_col = []\n",
        "  authorization_account_name_col = []\n",
        "  authorization_account_nuban_col = []\n",
        "  authorization_account_bank_col = []\n",
        "  authorization_account_created_at_col = []\n",
        "  authorization_account_last_updated_col = []\n",
        "  authorization_account_balance_col = []\n",
        "  authorization_account_customer_col = []\n",
        "  authorization_account_type_col = []\n",
        "  authorization_account_currency_col = []\n",
        "  authorization_accounts_col = []\n",
        "  authorization_amount_col = []\n",
        "  authorization_bank_col = []\n",
        "  authorization_created_at_col = []\n",
        "  authorization_currency_col = []\n",
        "  authorization_customerDetails_col = []\n",
        "  authorization_disconnect_col = []\n",
        "  authorization_disconnected_at_col = []\n",
        "  authorization_duration_col = []\n",
        "  authorization_env_col = []\n",
        "  authorization_garnish_col = []\n",
        "  authorization_initialAmount_col = []\n",
        "  authorization_initiated_col = []\n",
        "  authorization_last_updated_col = []\n",
        "  authorization_link_col = []\n",
        "  authorization_next_payment_col = []\n",
        "  authorization_owner_col = []\n",
        "  authorization_payLink_col = []\n",
        "  authorization_type_col = []\n",
        "  authorization_used_col = []\n",
        "  authorizationId_col = []\n",
        "  bankId_col = []\n",
        "  bankName_col = []\n",
        "  bankSlug_col = []\n",
        "  bankType_col = []\n",
        "  callbackURL_col = []\n",
        "  callback_code_col = []\n",
        "  callback_type_col = []\n",
        "  callback_url_col = []\n",
        "  code_col = []\n",
        "  country_col = []\n",
        "  current_project_col = []\n",
        "  customerEmail_col = []\n",
        "  customerId_col = []\n",
        "  ended_at_col = []\n",
        "  env_col = []\n",
        "  extras_col = []\n",
        "  identityType_col = []\n",
        "  login_type_col = []\n",
        "  message_col = []\n",
        "  meta_col = []\n",
        "  method_col = []\n",
        "  options_col = []\n",
        "  owner_col = []\n",
        "  record_col = []\n",
        "  recordId_col = []\n",
        "  started_at_col = []\n",
        "  status_webhook_col = []\n",
        "  token_col = []\n",
        "  type_col = []\n",
        "\n",
        "  list_column_none_okra_webhook = []\n",
        "  list_column_none_okra_webhook = [api_request_col, account_number_col, account_name_col, totalsent_col, \n",
        "                                   message_sms_payload_col, cost_col, status_col, responseCode_col,\n",
        "                                   bvn_col, requestSuccessful_col, responseMessage_col, email_col, phone_Col, \n",
        "                                   endpoint_Col, date_col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_Col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "  for index, row in df_okra.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "\n",
        "    # check if the row contains \"info\" string\n",
        "    if re.search('info', str_text):\n",
        "        log_level = re.search('info', str_text)\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)             \n",
        "        if 'mailto' not in str_text:\n",
        "            if re.search('OKRA WEBHOOK', str_text):\n",
        "                  type_of_request = re.search('OKRA WEBHOOK', str_text)\n",
        "                  okra_webhook = parse_wallet_sms_payload_success(str_text)                  \n",
        "                  accountId_col.append(okra_webhook.get('accountId'))\n",
        "                  authorization_v_col.append(okra_webhook.get('authorization').get('__v '))\n",
        "                  authorization_id_col.append(okra_webhook.get('authorization').get('_id '))\n",
        "                  authorization_customer_col.append(okra_webhook.get('authorization').get('customer '))\n",
        "                  authorization_account_col.append(okra_webhook.get('authorization').get('account '))\n",
        "                  authorization_account_id_col.append(okra_webhook.get('authorization').get('account ')[0].get('_id '))\n",
        "                  authorization_account_manual_col.append(okra_webhook.get('authorization').get('account ')[0].get('manual '))\n",
        "                  authorization_account_name_col.append(okra_webhook.get('authorization').get('account ')[0].get('name '))\n",
        "                  authorization_account_nuban_col.append(okra_webhook.get('authorization').get('account ')[0].get('nuban '))\n",
        "                  authorization_account_bank_col.append(okra_webhook.get('authorization').get('account ')[0].get('bank '))\n",
        "                  authorization_account_created_at_col.append(okra_webhook.get('authorization').get('account ')[0].get('created_at '))\n",
        "                  authorization_account_last_updated_col.append(okra_webhook.get('authorization').get('account ')[0].get('last_updated '))\n",
        "                  authorization_account_balance_col.append(okra_webhook.get('authorization').get('account ')[0].get('balance '))\n",
        "                  authorization_account_customer_col.append(okra_webhook.get('authorization').get('account ')[0].get('customer '))\n",
        "                  authorization_account_type_col.append(okra_webhook.get('authorization').get('account ')[0].get('type '))\n",
        "                  authorization_account_currency_col.append(okra_webhook.get('authorization').get('account ')[0].get('currency '))\n",
        "                  authorization_accounts_col.append(okra_webhook.get('authorization').get('accounts '))\n",
        "                  authorization_amount_col.append(okra_webhook.get('authorization').get('amount '))\n",
        "                  authorization_bank_col.append(okra_webhook.get('authorization').get('bank '))\n",
        "                  authorization_created_at_col.append(okra_webhook.get('authorization').get('created_at '))\n",
        "                  authorization_currency_col.append(okra_webhook.get('authorization').get('currency '))\n",
        "                  authorization_customerDetails_col.append(okra_webhook.get('authorization').get('customerDetails '))\n",
        "                  authorization_disconnect_col.append(okra_webhook.get('authorization').get('disconnect '))\n",
        "                  authorization_disconnected_at_col.append(okra_webhook.get('authorization').get('disconnected_at '))\n",
        "                  authorization_duration_col.append(okra_webhook.get('authorization').get('duration '))\n",
        "                  authorization_env_col.append(okra_webhook.get('authorization').get('env '))\n",
        "                  authorization_garnish_col.append(okra_webhook.get('authorization').get('garnish '))\n",
        "                  authorization_initialAmount_col.append(okra_webhook.get('authorization').get('initialAmount '))\n",
        "                  authorization_initiated_col.append(okra_webhook.get('authorization').get('initiated '))\n",
        "                  authorization_last_updated_col.append(okra_webhook.get('authorization').get('last_updated '))\n",
        "                  authorization_link_col.append(okra_webhook.get('authorization').get('link '))\n",
        "                  authorization_next_payment_col.append(okra_webhook.get('authorization').get('next_payment '))\n",
        "                  authorization_owner_col.append(okra_webhook.get('authorization').get('owner '))\n",
        "                  authorization_payLink_col.append(okra_webhook.get('authorization').get('payLink '))\n",
        "                  authorization_type_col.append(okra_webhook.get('authorization').get('type '))\n",
        "                  authorization_used_col.append(okra_webhook.get('authorization').get('used '))\n",
        "                  authorizationId_col.append(okra_webhook.get('authorizationId'))\n",
        "                  bankId_col.append(okra_webhook.get('bankId'))\n",
        "                  bankName_col.append(okra_webhook.get('bankName'))\n",
        "                  bankSlug_col.append(okra_webhook.get('bankSlug'))\n",
        "                  bankType_col.append(okra_webhook.get('bankType'))\n",
        "                  callbackURL_col.append(okra_webhook.get('callbackURL'))\n",
        "                  callback_code_col.append(okra_webhook.get('callback_code'))\n",
        "                  callback_type_col.append(okra_webhook.get('callback_type'))\n",
        "                  callback_url_col.append(okra_webhook.get('callback_url'))\n",
        "                  code_col.append(okra_webhook.get('code'))\n",
        "                  country_col.append(okra_webhook.get('country'))\n",
        "                  current_project_col.append(okra_webhook.get('current_project'))\n",
        "                  customerEmail_col.append(okra_webhook.get('customerEmail'))\n",
        "                  customerId_col.append(okra_webhook.get('customerId'))\n",
        "                  ended_at_col.append(okra_webhook.get('ended_at'))\n",
        "                  env_col.append(okra_webhook.get('env'))\n",
        "                  extras_col.append(okra_webhook.get('extras'))\n",
        "                  identityType_col.append(okra_webhook.get('identityType'))\n",
        "                  login_type_col.append(okra_webhook.get('login_type'))\n",
        "                  message_col.append(okra_webhook.get('message'))\n",
        "                  meta_col.append(okra_webhook.get('meta'))\n",
        "                  method_col.append(okra_webhook.get('method'))\n",
        "                  options_col.append(okra_webhook.get('options'))\n",
        "                  owner_col.append(okra_webhook.get('owner'))\n",
        "                  record_col.append(okra_webhook.get('record'))\n",
        "                  recordId_col.append(okra_webhook.get('recordId'))\n",
        "                  started_at_col.append(okra_webhook.get('started_at'))\n",
        "                  status_webhook_col.append(okra_webhook.get('status'))\n",
        "                  token_col.append(okra_webhook.get('token'))\n",
        "                  type_col.append(okra_webhook.get('type'))\n",
        "\n",
        "                  try:\n",
        "                    type_request_col.append(type_of_request.group(0))\n",
        "                  except AttributeError:\n",
        "                    type_request_col.append(None)\n",
        "                  \n",
        "                  for n in range(len(list_column_none_okra_webhook)):\n",
        "                    list_column_none_okra_webhook[n].append(None)\n",
        "\n",
        "                     \n",
        "        elif re.search('OKRA PAYLOAD', str_text): # Nothing\n",
        "          type_of_request = re.search('OKRA PAYLOAD', str_text)\n",
        "        elif re.search('OKRA SUCCESS', str_text):   # Nothing\n",
        "          type_of_request = re.search('OKRA SUCCESS', str_text)\n",
        "        elif re.search('VTPASS SUCCESS', str_text):   # Nothing\n",
        "          type_of_request = re.search('VTPASS SUCCESS', str_text)  \n",
        "\n",
        "       \n",
        "  df_okra['Type_Request'] = type_request_col\n",
        "  df_okra['Phone_Number'] = phone_Col\n",
        "  df_okra['Date'] = date_col\n",
        "  df_okra['EndPoint'] = endpoint_Col\n",
        "  df_okra['Log_Level'] = log_level_col\n",
        "  df_okra['Email'] = email_col\n",
        "  df_okra['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_okra['Total Sent'] = totalsent_col\n",
        "  df_okra['Cost'] = cost_col\n",
        "  df_okra['Status'] = status_col\n",
        "  df_okra['Account Number'] = account_number_col\n",
        "  df_okra['Account Name'] = account_name_col\n",
        "  df_okra['BVN'] = bvn_col\n",
        "  df_okra['Request Successful'] = requestSuccessful_col\n",
        "  df_okra['Response Message'] = responseMessage_col\n",
        "  df_okra['Response Code'] = responseCode_col\n",
        "  df_okra['Account Id'] = accountId_col\n",
        "  df_okra['Authorization_V'] = authorization_v_col\n",
        "  df_okra['Authorization_Id'] = authorization_id_col\n",
        "  df_okra['Authorization_Customer'] = authorization_customer_col\n",
        "  df_okra['Authorization_Owner'] = authorization_owner_col\n",
        "  df_okra['Authorization_Account'] = authorization_account_col\n",
        "  df_okra['Authorization_account_Id'] = authorization_account_id_col\n",
        "  df_okra['Authorization_account_manual'] = authorization_account_manual_col\n",
        "  df_okra['Authorization_account_name'] = authorization_account_name_col\n",
        "  df_okra['Authorization_account_nuban'] = authorization_account_nuban_col\n",
        "  df_okra['Authorization_account_bank'] = authorization_account_bank_col\n",
        "  df_okra['Authorization_account_created_at'] = authorization_account_created_at_col\n",
        "  df_okra['Authorization_account_last_updated'] = authorization_account_last_updated_col\n",
        "  df_okra['Authorization_account_balance'] = authorization_account_balance_col\n",
        "  df_okra['Authorization_account_customer'] = authorization_account_customer_col\n",
        "  df_okra['Authorization_account_type'] = authorization_account_type_col\n",
        "  df_okra['Authorization_account_currency'] = authorization_account_currency_col\n",
        "  df_okra['Authorization_accounts'] = authorization_accounts_col\n",
        "  df_okra['Authorization_amount'] = authorization_amount_col\n",
        "  df_okra['Authorization_bank'] = authorization_bank_col\n",
        "  df_okra['Authorization_created_at'] = authorization_created_at_col\n",
        "  df_okra['Authorization_currency'] = authorization_currency_col \n",
        "  df_okra['Authorization_customerDetails'] = authorization_customerDetails_col\n",
        "  df_okra['Authorization_disconnect'] = authorization_disconnect_col\n",
        "  df_okra['Authorization_disconnected_at'] = authorization_disconnected_at_col\n",
        "  df_okra['Authorization_duration'] = authorization_duration_col\n",
        "  df_okra['Authorization_env'] = authorization_env_col\n",
        "  df_okra['Authorization_garnish'] = authorization_garnish_col\n",
        "  df_okra['Authorization_initialAmount'] = authorization_initialAmount_col\n",
        "  df_okra['Authorization_initiated'] = authorization_initiated_col\n",
        "  df_okra['Authorization_last_updated'] = authorization_last_updated_col\n",
        "  df_okra['Authorization_link'] = authorization_link_col\n",
        "  df_okra['Authorization_next_payment'] = authorization_next_payment_col\n",
        "  df_okra['Authorization_payLink'] = authorization_payLink_col\n",
        "  df_okra['Authorization_type'] = authorization_type_col\n",
        "  df_okra['Authorization_used'] = authorization_used_col\n",
        "  df_okra['AuthorizationId'] = authorizationId_col\n",
        "  df_okra['BankId'] = bankId_col\n",
        "  df_okra['BankName'] = bankName_col\n",
        "  df_okra['bankSlug'] = bankSlug_col\n",
        "  df_okra['bankType'] = bankType_col\n",
        "  df_okra['callbackURL'] = callbackURL_col\n",
        "  df_okra['callback_code'] = callback_code_col\n",
        "  df_okra['callback_type'] = callback_type_col \n",
        "  df_okra['callback_url'] = callback_url_col\n",
        "  df_okra['code'] = code_col\n",
        "  df_okra['country'] = country_col\n",
        "  df_okra['current_project'] = current_project_col\n",
        "  df_okra['customerEmail'] = customerEmail_col\n",
        "  df_okra['customerId'] = customerId_col\n",
        "  df_okra['ended_at'] = ended_at_col\n",
        "  df_okra['env'] = env_col\n",
        "  df_okra['extras'] = extras_col\n",
        "  df_okra['identityType'] = identityType_col\n",
        "  df_okra['login_type'] = login_type_col\n",
        "  df_okra['message'] = message_col\n",
        "  df_okra['meta'] = meta_col\n",
        "  df_okra['method'] = method_col\n",
        "  df_okra['options'] = options_col\n",
        "  df_okra['owner'] = owner_col\n",
        "  df_okra['record'] = record_col \n",
        "  df_okra['recordId'] = recordId_col\n",
        "  df_okra['started_at'] = started_at_col\n",
        "  df_okra['status_webhook'] = status_webhook_col\n",
        "  df_okra['token'] = token_col\n",
        "  df_okra['type'] = type_col\n",
        " \n",
        "  return df_okra\n"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW4ZjQeK3Z10"
      },
      "source": [
        "df_okra_webhook = df_raw[df_raw['text'].str.contains('OKRA WEBHOOK')]\n",
        "parse_row_okra_webhook_function(df_okra_webhook)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbWmCvYp3SXf"
      },
      "source": [
        ""
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPh9NrF2OND7"
      },
      "source": [
        "### Handle DataFrame for LEADWAY SUCCESS Type request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWOCepv9ONYw"
      },
      "source": [
        "import re\n",
        "import regex\n",
        "import json\n",
        "\n",
        "def parse_row_leadway_function(df_leadway_success):\n",
        "  log_level_col = []\n",
        "  api_request_col = []\n",
        "  type_request_col = []\n",
        "  phone_Col = []\n",
        "  date_col = []\n",
        "  endpoint_Col = []\n",
        "  email_col = []\n",
        "  message_sms_payload_col = []\n",
        "  totalsent_col = []\n",
        "  cost_col = []\n",
        "  status_col = []\n",
        "  account_number_col = []\n",
        "  account_name_col = []\n",
        "  bvn_col = []\n",
        "  requestSuccessful_col = []\n",
        "  responseMessage_col = []\n",
        "  responseCode_col = []\n",
        "\n",
        "  list_column_none_leadway_success = []\n",
        "  list_column_none_leadway_success = [message_sms_payload_col, totalsent_col, cost_col, status_col, \n",
        "                                     account_number_col, bvn_col, requestSuccessful_col, responseMessage_col,\n",
        "                                     responseCode_col, account_name_col, email_col, phone_Col, endpoint_Col, date_col]\n",
        "  \n",
        "  list_all_colum = []\n",
        "  list_all_colum = [type_request_col, phone_Col, date_col, endpoint_Col, log_level_col, email_col, \n",
        "                    message_sms_payload_col, totalsent_col, cost_col, status_col, account_number_col,\n",
        "                    account_name_col, bvn_col, requestSuccessful_col, responseMessage_col, responseCode_col]\n",
        "\n",
        "  for index, row in df_leadway_success.iterrows():\n",
        "    str_text = row['text']\n",
        "    \n",
        "    if not str_text.startswith('['):\n",
        "      for i in range(len(list_all_colum)):\n",
        "          list_all_colum[i].append(None)\n",
        "\n",
        "    # check if the row contains \"info\" string\n",
        "    if re.search('info', str_text):\n",
        "        log_level = re.search('info', str_text)\n",
        "        try:\n",
        "          log_level_col.append(log_level.group(0))\n",
        "        except AttributeError:\n",
        "          log_level_col.append(None)             \n",
        "        if 'mailto' not in str_text:\n",
        "            if re.search('LEADWAY SUCCESS', str_text):\n",
        "                  type_of_request = re.search('LEADWAY SUCCESS', str_text)\n",
        "                  leadway_success_concat_text, index_first_succ, index_last_succ = parse_and_concatenate_Leadway_Success_Rows(df_)\n",
        "                  res_text_leadway = parse_Leadway_Success_Row(leadway_success_concat_text)\n",
        "                  for o in range(len(list_column_none_leadway_success)):\n",
        "                    list_column_none_leadway_success[o].append(None)\n",
        "\n",
        "                  try:\n",
        "                    type_request_col.append(type_of_request.group(0))\n",
        "                  except AttributeError:\n",
        "                    type_request_col.append(None)\n",
        "                   \n",
        "        elif re.search('OKRA PAYLOAD', str_text): # Nothing\n",
        "          type_of_request = re.search('OKRA PAYLOAD', str_text)\n",
        "        elif re.search('OKRA SUCCESS', str_text):   # Nothing\n",
        "          type_of_request = re.search('OKRA SUCCESS', str_text)\n",
        "        elif re.search('VTPASS SUCCESS', str_text):   # Nothing\n",
        "          type_of_request = re.search('VTPASS SUCCESS', str_text)  \n",
        "\n",
        "  df_leadway_success['Type_Request'] = type_request_col\n",
        "  df_leadway_success['Phone_Number'] = phone_Col\n",
        "  df_leadway_success['Date'] = date_col\n",
        "  df_leadway_success['EndPoint'] = endpoint_Col\n",
        "  df_leadway_success['Log_Level'] = log_level_col\n",
        "  df_leadway_success['Email'] = email_col\n",
        "  df_leadway_success['Message SMS Payload'] = message_sms_payload_col\n",
        "  df_leadway_success['Total Sent'] = totalsent_col\n",
        "  df_leadway_success['Cost'] = cost_col\n",
        "  df_leadway_success['Status'] = status_col\n",
        "  df_leadway_success['Account Number'] = account_number_col\n",
        "  df_leadway_success['Account Name'] = account_name_col\n",
        "  df_leadway_success['BVN'] = bvn_col\n",
        "  df_leadway_success['Request Successful'] = requestSuccessful_col\n",
        "  df_leadway_success['Response Message'] = responseMessage_col\n",
        "  df_leadway_success['Response Code'] = responseCode_col\n",
        " \n",
        "  return df_leadway_success\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFTSv_Y0mcjO"
      },
      "source": [
        "### Concatenate DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-VL2Igymfck"
      },
      "source": [
        "pdList = [df_error, resutat_df_api, resutat_df_client_mobile_login, resutat_df_sms_payload, \n",
        "          resutat_df_sms_success, resutat_df_wallet_success, df_okra_webhook]  # List of our dataframes\n",
        "df_final = pd.concat(pdList)\n",
        "df_final"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qsXB16xUGyd"
      },
      "source": [
        "### Export final Dataframe to a file csv."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyP128bIS8o1"
      },
      "source": [
        "df_final.to_csv('/content/drive/MyDrive/datasets/nirra_log_bot.csv', index=None)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xwMHt6A2r8fL",
        "outputId": "44ab2dcc-a91f-45b2-ad2a-70c07beb60d2"
      },
      "source": [
        "len(df_final['Phone_Number'].unique())"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "116"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tAOylN7OU99"
      },
      "source": [
        "### Récupérer les numéros de téléphone différents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z47mOjmAOkCM"
      },
      "source": [
        "df_final['Phone_Number'].unique() # récupérer la liste des différents numéros de téléphone\n",
        "all_users_phone_number = [element for element in df_final['Phone_Number'].unique() if element != None] # all userId\n",
        "all_users_phone_number # liste de tous les différents numéros de téléphone.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOulnkEKP-Wr"
      },
      "source": [
        "all_df_phone_number = [] # to get list of dataframes\n",
        "for phoneNumber in all_users_phone_number:\n",
        "  df_phone_number = df_final[df_final['Phone_Number'] == phoneNumber] # récupérer les dataframes avec seulement les numéros de téléphone\n",
        "  all_df_phone_number.append(df_phone_number)  # mettre chaque dataframe dans la\n",
        "\n",
        "all_df_phone_number[0] # print the first dataframe inside the list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_tvM7PdRSE0"
      },
      "source": [
        "### sauvegarder les dataframes sur disque "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq3drem3RRm2",
        "outputId": "7ecd34df-40f4-49f5-e018-c926b36c6c98"
      },
      "source": [
        "from tqdm import tqdm\n",
        "# tqdm permet\n",
        "for phoneNumber in tqdm(all_users_phone_number):\n",
        "  df_phone_number = df_final[df_final['Phone_Number'] == phoneNumber]\n",
        "  df_phone_number.to_csv(f'/content/drive/MyDrive/datasets/files/{phoneNumber}.csv', index=None)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 115/115 [00:01<00:00, 65.60it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNwcH2kFnpiP",
        "outputId": "2891b399-a64d-430b-c2e7-0caecf64a1bc"
      },
      "source": [
        "print(len(all_users_phone_number))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cct87UPHuM_7",
        "outputId": "25d35549-ba8a-4056-effa-656ea17869a5"
      },
      "source": [
        "cd /content/drive/MyDrive/datasets/files/"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/datasets/files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGN3PMQmuSot"
      },
      "source": [
        "def isNaN(string):\n",
        "    return string != string"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YixcW0AMuTYZ",
        "outputId": "43fef611-975a-4c80-9e35-9f776adc3795"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "from pandas.plotting import scatter_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# use glob to get all the csv files \n",
        "# in the folder\n",
        "path = os.getcwd()\n",
        "csv_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
        "all_df_list = [] # list of all dataframe\n",
        "# loop over the list of csv files\n",
        "number_transactions = []\n",
        "all_df_list_with_number_transaction = []\n",
        "all_df_with_diff_ts = []\n",
        "\n",
        "for f in tqdm(csv_files):    \n",
        "    # read the csv file\n",
        "    df = pd.read_csv(f)  \n",
        "    all_df_list.append(df)   \n",
        "\n",
        "for element in all_df_list:\n",
        "  element['Number_Transactions'] = element.shape[0] # faire la somme des lignes et mettre dans la colonne Number_Transactions\n",
        "  all_df_list_with_number_transaction.append(element)# mettre tous les éléments dans une nouvelle liste\n",
        "\n",
        "#Faire la différence entre les timestamp\n",
        "for ele_with_num_trans in all_df_list_with_number_transaction:\n",
        "  df_ts = pd.DataFrame(ele_with_num_trans['ts'])\n",
        "  ele_with_num_trans['ts_diff'] = df_ts.diff(axis=0)\n",
        "  all_df_with_diff_ts.append(ele_with_num_trans)\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 115/115 [00:01<00:00, 92.45it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eiTbqq3uQjd",
        "outputId": "066ee69b-c65b-4b30-dd4e-bc4231d2d20d"
      },
      "source": [
        "print(len(all_df_list_with_number_transaction))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13HqVdxVugXu",
        "outputId": "efff3c2d-2111-49b4-a655-75070a092d9c"
      },
      "source": [
        "print(len(all_df_with_diff_ts))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9pqKluVvIo4"
      },
      "source": [
        "all_df_with_diff_ts[50].head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtRP6naGtge1"
      },
      "source": [
        "### Faire le plot en fonction du nombre des transactions et de la différence entre timestamp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MghFTVudtc6O"
      },
      "source": [
        "# Faire des plots pour chaque utilisateur\n",
        "for ele_plot in all_df_with_diff_ts:\n",
        "  attributes = [\"Number_Transactions\", \"ts_diff\"]\n",
        "  #scatter_matrix(ele_plot[attributes], figsize=(10, 5))\n",
        "  #plt.show()"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnGwIcUmR9yO"
      },
      "source": [
        "### Création des times series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGmlFq2QtTa1",
        "outputId": "8c746446-8da0-47cd-cfa8-3de5b3a28f79"
      },
      "source": [
        "all_df_res = []\n",
        "new_endpoint_col = []\n",
        "for new_data_frame in tqdm(all_df_with_diff_ts):\n",
        "  new_data_frame['EndPoint'].fillna('', inplace=True) # modifie la colonne\n",
        "\n",
        "  new_data_frame['Appplication_get'] = new_data_frame['EndPoint'].str.contains('applications/get').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Payment'] = new_data_frame['EndPoint'].str.contains('payment').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Declined'] = new_data_frame['EndPoint'].str.contains('decline').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Preapprouved'] = new_data_frame['EndPoint'].str.contains('preapproved').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Accepted'] = new_data_frame['EndPoint'].str.contains('accept').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Lifestyle'] = new_data_frame['EndPoint'].str.contains('lifestyle').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Wallet_balance'] = new_data_frame['EndPoint'].str.contains('wallet/balance').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Adverts'] = new_data_frame['EndPoint'].str.contains('adverts').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Bank'] = new_data_frame['EndPoint'].str.contains('bank').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Updated'] = new_data_frame['EndPoint'].str.contains('update').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Products'] = new_data_frame['EndPoint'].str.contains('products').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Locations'] = new_data_frame['EndPoint'].str.contains('locations').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Client_Get'] = new_data_frame['EndPoint'].str.contains('client/get').astype(np.float).astype('Int32')\n",
        "  new_data_frame['Client_V2_Create'] = new_data_frame['EndPoint'].str.contains('client/v2/create').astype(np.float).astype('Int32')\n",
        "  \n",
        "  \n",
        "  new_data_frame['Dummy'] = 0\n",
        "  new_data_frame['Payment_cumulative'] = new_data_frame.groupby(['Dummy'])['Payment'].cumsum()\n",
        "  new_data_frame['Decline_cumulative'] = new_data_frame.groupby(['Dummy'])['Declined'].cumsum()\n",
        "  new_data_frame['Application_get_cumulative'] = new_data_frame.groupby(['Dummy'])['Appplication_get'].cumsum()\n",
        "  new_data_frame['Preapprouved_cumulative'] = new_data_frame.groupby(['Dummy'])['Preapprouved'].cumsum()\n",
        "  new_data_frame['Accepted_cumulative'] = new_data_frame.groupby(['Dummy'])['Accepted'].cumsum()\n",
        "  new_data_frame['Lifestyle_cumulative'] = new_data_frame.groupby(['Dummy'])['Lifestyle'].cumsum()\n",
        "  new_data_frame['Wallet_balance_cumulative'] = new_data_frame.groupby(['Dummy'])['Wallet_balance'].cumsum()\n",
        "  new_data_frame['Adverts_cumulative'] = new_data_frame.groupby(['Dummy'])['Adverts'].cumsum()\n",
        "  new_data_frame['Bank_cumulative'] = new_data_frame.groupby(['Dummy'])['Bank'].cumsum()\n",
        "  new_data_frame['Update_cumulative'] = new_data_frame.groupby(['Dummy'])['Updated'].cumsum()\n",
        "  new_data_frame['Products_cumulative'] = new_data_frame.groupby(['Dummy'])['Products'].cumsum()\n",
        "  new_data_frame['Locations_cumulative'] = new_data_frame.groupby(['Dummy'])['Locations'].cumsum()\n",
        "  new_data_frame['Client_Get_cumulative'] = new_data_frame.groupby(['Dummy'])['Client_Get'].cumsum()\n",
        "  new_data_frame['Client_Get_cumulative'] = new_data_frame.groupby(['Dummy'])['Client_V2_Create'].cumsum()\n",
        "  all_df_res.append(new_data_frame)\n",
        "        "
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 115/115 [00:06<00:00, 18.40it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nJNZz1DrQT1",
        "outputId": "d5037ee2-5164-4e09-b585-ae7b77ff50c0"
      },
      "source": [
        "print(len(all_df_list))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr4jNrsyPMQB",
        "outputId": "eb9e3782-a412-46fc-e8ca-cb80f194e8c3"
      },
      "source": [
        "print(len(all_df_res))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjxF2xJfhUeo",
        "outputId": "c51a479d-6cb3-4774-ef56-037519d8ab21"
      },
      "source": [
        "cd /content/drive/My Drive/datasets/new_files"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/datasets/new_files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qebtQVU8k2hr"
      },
      "source": [
        "### Sauvegarder les nouveaux datasets dans un nouveau dossier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXcimjuqS7ya",
        "outputId": "ee791d34-d079-4c2b-d415-56f4350b4e08"
      },
      "source": [
        "from tqdm import tqdm\n",
        " #tqdm permet\n",
        "for elem in tqdm(all_df_res):\n",
        "  phone = elem['Phone_Number'][0]\n",
        "  elem.to_csv(f'/content/drive/MyDrive/datasets/new_files/{phone}.csv', index=None)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 115/115 [00:02<00:00, 53.26it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMyqOOEgAxNC"
      },
      "source": [
        "### Parcourir le dossier contenant les dataframes et convertir dans une nouvelle colonne \"TS_to_DateTime\" la colonne \"ts\" en Datetime et mettre dans la liste all_df_list_new"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8-UByHfAgI1"
      },
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def list_load_all_df(path='/content/drive/My Drive/datasets/new_files'):\n",
        "  #path = os.getcwd()\n",
        "  csv_files_new = glob.glob(os.path.join(path, \"*.csv\"))\n",
        "  all_df_list_new = []\n",
        "  # loop over the list of csv files\n",
        "  for f in tqdm(csv_files_new):    \n",
        "      # read the csv file\n",
        "      df_new = pd.read_csv(f)\n",
        "      df_new['TS_to_DateTime'] = df_new['ts'].apply(lambda x:datetime.fromtimestamp(x))\n",
        "      all_df_list_new.append(df_new)\n",
        "  return all_df_list_new"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sFPFzNoODr9"
      },
      "source": [
        "#cd .."
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyTYbdQyOPYs"
      },
      "source": [
        "#cd /content/drive/My Drive/datasets/files_with_another_fields"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZPxTbPYUPUr"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Récupérer le nombre d'opérations consécutives (opérations qui se déroulent dans l'interval de 5 min)\n",
        "def df_nombre_operations_consecutives(each_df_2):\n",
        "  '''\n",
        "    cette fonction prend comme paramètre un dataframe et retourne un dataframe avec les nouvelles colonnes\n",
        "  '''\n",
        "  each_df = each_df_2.copy() \n",
        "  size_df = each_df.shape[0]\n",
        "  ts_delta=0\n",
        "  count = 0\n",
        "  last_ts = 0\n",
        "  number_consecutive_operations = []\n",
        "  time_consecutive_operations = []\n",
        "  phone_number_each_df = []\n",
        "  # on récupère la 1ere date ou la 1ère lige de la colonne \"TS_to_DateTime\"\n",
        "  start_ts = each_df.iloc[0]['TS_to_DateTime']\n",
        "  result = pd.DataFrame(columns=['Phone_Number', 'Number_consecutive_operations', 'Time_consecutive_operations',\n",
        "                                                                'Number_operations_per_minute', 'Max_number_operation_per_minute',\n",
        "                                                                'Min_number_operation_per_minute', 'Mean_operation_per_minute',\n",
        "                                                                'Median_operation_per_minute'])\n",
        "  \n",
        "  for i in range(size_df-1):\n",
        "    ts_delta = each_df.iloc[i+1]['TS_to_DateTime'] - each_df.iloc[i]['TS_to_DateTime']\n",
        "    # si l'interval de temps pendant lequel les opérations se sont effectuées est <= à 5 min \n",
        "    if (ts_delta.total_seconds()/60) <= 5:\n",
        "      last_ts = each_df.iloc[i+1]['TS_to_DateTime']\n",
        "      # on compte les opérations effectuées en 5 minutes\n",
        "      count +=1\n",
        "    else :\n",
        "      last_ts = each_df.iloc[i+1]['TS_to_DateTime']\n",
        "      ts_delta = (last_ts - start_ts).total_seconds()/60\n",
        "      # liste des temps en minutes pendant lequel les opérations se sont déroulées\n",
        "      time_consecutive_operations.append(ts_delta)\n",
        "      # nombres d'opérations consécutives (c-à-d opérations qui se déroulées dans un interval de 5 min)\n",
        "      number_consecutive_operations.append(count)\n",
        "      phone_number_each_df.append(each_df.iloc[i+1]['Phone_Number'])\n",
        "      count = 0\n",
        "      start_ts = each_df.iloc[i+1]['TS_to_DateTime']\n",
        "      last_ts = each_df.iloc[i+1]['TS_to_DateTime']\n",
        "\n",
        "      df_phone_number_each_df = pd.DataFrame(phone_number_each_df)  \n",
        "      df_number_consecutive_operations = pd.DataFrame(number_consecutive_operations)\n",
        "      df_time_consecutive_operations = pd.DataFrame(time_consecutive_operations)\n",
        "      pdList_consecutive_operations = [df_phone_number_each_df, df_number_consecutive_operations, df_time_consecutive_operations] # List of our dataframes\n",
        "      df_final_consecutive_operations = pd.concat(pdList_consecutive_operations, axis=1)\n",
        "      df_final_consecutive_operations.columns=['Phone_Number', 'Number_consecutive_operations', 'Time_consecutive_operations']\n",
        "      df_final_consecutive_operations['Number_operations_per_minute'] = df_final_consecutive_operations['Number_consecutive_operations']/df_final_consecutive_operations['Time_consecutive_operations']\n",
        "      df_final_consecutive_operations['Max_number_operation_per_minute'] = df_final_consecutive_operations['Number_operations_per_minute'].max()\n",
        "      df_final_consecutive_operations['Min_number_operation_per_minute'] = df_final_consecutive_operations['Number_operations_per_minute'].min()\n",
        "      df_final_consecutive_operations['Mean_operation_per_minute'] = df_final_consecutive_operations['Number_operations_per_minute'].mean()\n",
        "      df_final_consecutive_operations['Median_operation_per_minute'] = df_final_consecutive_operations['Number_operations_per_minute'].median()\n",
        "      result = df_final_consecutive_operations\n",
        "  return result\n"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emYiv5tEgPMp"
      },
      "source": [
        "def df_one_operation(each_df):\n",
        "\n",
        "  copy_df = each_df.copy()\n",
        "  copy_df['Number_consecutive_operations'] = 0\n",
        "  copy_df['Time_consecutive_operations'] = 0\n",
        "  copy_df['Number_operations_per_minute'] = 0\n",
        "  copy_df['Max_number_operation_per_minute'] = 0\n",
        "  copy_df['Min_number_operation_per_minute'] = 0\n",
        "  copy_df['Mean_operation_per_minute'] = 0\n",
        "  copy_df['Median_operation_per_minute'] = 0\n",
        "  return copy_df['Phone_Number'][0]\n"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsTipO3qOSFX",
        "outputId": "21321f68-c9e7-4ff5-8a6d-eb7fb64467cc"
      },
      "source": [
        "res_all_df = list_load_all_df()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 115/115 [00:01<00:00, 68.98it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0S0aVk1dPKpw"
      },
      "source": [
        "print(res_all_df[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNJz3vRZTkk0",
        "outputId": "b5b64c77-bc93-4745-9720-56e23c445be6"
      },
      "source": [
        "res_all_df[0]['Phone_Number'][0]"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7035864882"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC5ekL9xS_-R"
      },
      "source": [
        "test_variable = [el for el in res_all_df if el['Phone_Number'][0] == 2348084487979 ][0]"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0Z3mimYURv9"
      },
      "source": [
        "test_variable"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "id": "m8FHIT4yUds2",
        "outputId": "e0f70253-f962-4ca1-d129-f71823d324b5"
      },
      "source": [
        "df_nombre_operations_consecutives(test_variable)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Phone_Number</th>\n",
              "      <th>Number_consecutive_operations</th>\n",
              "      <th>Time_consecutive_operations</th>\n",
              "      <th>Number_operations_per_minute</th>\n",
              "      <th>Max_number_operation_per_minute</th>\n",
              "      <th>Min_number_operation_per_minute</th>\n",
              "      <th>Mean_operation_per_minute</th>\n",
              "      <th>Median_operation_per_minute</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2348084487979</td>\n",
              "      <td>0</td>\n",
              "      <td>127.98317</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Phone_Number  ...  Median_operation_per_minute\n",
              "0  2348084487979  ...                          0.0\n",
              "\n",
              "[1 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5PvhDc0iTYM"
      },
      "source": [
        "res_all_df[4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z97OZ5UoGqWL"
      },
      "source": [
        "_list_all_df = []\n",
        "for element in res_all_df:\n",
        "  # pour debugger il faut utiliser try -  except et utiliser pass\n",
        "  # ainsi, en cas de problème on pourrait savoir dans quel DataFrame il se trouve\n",
        "  if element.shape[0] > 1:\n",
        "    _list_all_df.append(df_nombre_operations_consecutives(element))\n",
        "    \n",
        "  \n",
        " "
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGrDo644nEDO"
      },
      "source": [
        "list_all_df = []\n",
        "for element in res_all_df:\n",
        "  if element.shape[0] <= 1:\n",
        "    list_all_df.append(df_one_operation(element))"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hr12W1IMhk4w"
      },
      "source": [
        "print(len(_list_all_df))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7ecVPn8nddg"
      },
      "source": [
        "list_all_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVB77ev8OxrL"
      },
      "source": [
        "print(_list_all_df[6])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "id": "m_jXIwx_WzIy",
        "outputId": "3ba65b79-fb19-4544-cbda-9fd353cc8cbb"
      },
      "source": [
        "_list_all_df[1]"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Phone_Number</th>\n",
              "      <th>Number_consecutive_operations</th>\n",
              "      <th>Time_consecutive_operations</th>\n",
              "      <th>Number_operations_per_minute</th>\n",
              "      <th>Max_number_operation_per_minute</th>\n",
              "      <th>Min_number_operation_per_minute</th>\n",
              "      <th>Mean_operation_per_minute</th>\n",
              "      <th>Median_operation_per_minute</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8134676606</td>\n",
              "      <td>4</td>\n",
              "      <td>285.850928</td>\n",
              "      <td>0.013993</td>\n",
              "      <td>0.013993</td>\n",
              "      <td>0.013993</td>\n",
              "      <td>0.013993</td>\n",
              "      <td>0.013993</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Phone_Number  ...  Median_operation_per_minute\n",
              "0    8134676606  ...                     0.013993\n",
              "\n",
              "[1 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0TAQIWLGr3y"
      },
      "source": [
        "def give_dataframe(phone_number, path_df='/content/drive/MyDrive/datasets/new_files'):\n",
        "  df = pd.read_csv(f'{path_df}/{phone_number}.csv')\n",
        "  df['TS_to_DateTime'] = df['ts'].apply(lambda x:datetime.fromtimestamp(x))\n",
        "  return df"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KovtgswlLeye"
      },
      "source": [
        "def give_dataframe_2(path_complet_df='/content/drive/MyDrive/datasets/new_files/2349152152920.csv'):\n",
        "  df_2 = pd.read_csv(path_complet_df)\n",
        "  df_2['TS_to_DateTime'] = df_2['ts'].apply(lambda x:datetime.fromtimestamp(x))\n",
        "  return df_2"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAE3fiC5Mbke"
      },
      "source": [
        ""
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4VoVXkeHiRR"
      },
      "source": [
        "df_test_function = give_dataframe('2349024868556')"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbXVF8bgK_p3"
      },
      "source": [
        ""
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-CJceBZA6Q-"
      },
      "source": [
        "from tqdm import tqdm\n",
        " #tqdm permet\n",
        "for elem in tqdm(resultat_df_list):\n",
        "  phone = elem['Phone_Number'][0]\n",
        "  elem.to_csv(f'/content/drive/MyDrive/datasets/files_with_another_fields/{phone}.csv', index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-fzLc7xU0wm"
      },
      "source": [
        "### Liste des opérations par utilisateur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yiCSXGcTA0e"
      },
      "source": [
        "### NB: tenir compte du format (float) du dénominateur au moment de faire la division."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nVEiYCjdice"
      },
      "source": [
        "### extraction d'une  métrique pour le nombre d'opérations effectuées par un utilisateur sur la base de l'endpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHzbosvqfDl9"
      },
      "source": [
        "# liste des personnes à risque\n",
        "list_personnes_risque = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwgcKD2YF3Rs"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_qaUJpIEvml"
      },
      "source": [
        "import statistics\n",
        "\n",
        "def calculate_metric(list_df, str_key_word='/client/'):\n",
        "  '''cette fonction permet de calculer la moyenne, la mediane, la valeur min, max et le seuil\n",
        "    en se référant sur la liste contenant la somme des opérations effectuées dans chaque dataframe. ''' \n",
        "  sum_operation = []\n",
        "  list_phone_number_user = []\n",
        "  for one_df in list_df:\n",
        "    one_df['EndPoint'].fillna('', inplace=True)\n",
        "    # tenir compte du cast d'un boolean en float avec la propriété astype(float)\n",
        "    sum_operation.append(one_df['EndPoint'].str.contains(str_key_word).astype(float).sum())\n",
        "    list_phone_number_user.append(one_df['Phone_Number'][0])\n",
        "\n",
        "  mini = min(sum_operation)\n",
        "  maxi = max(sum_operation)\n",
        "  mediane = statistics.median(sum_operation)\n",
        "  moyenne = statistics.mean(sum_operation)\n",
        "  ecart_type = statistics.pstdev(sum_operation)\n",
        "  threshold = mediane + (3*ecart_type) # les valeurs au délà de 3* écart type sont considérées comme outliers, on considère cette valeur comme notre seuil\n",
        "  print(f' print 3 écart-type: {3*ecart_type}')\n",
        "  print(f'somme operation est: {sum_operation}')\n",
        "  print(f'la liste des numéros de téléphone des utilisateur est: {list_phone_number_user}')\n",
        "  print(f'le max est: {maxi}\\nle min est: {mini}\\nla mediane est: {mediane}\\nla moyenne est {moyenne}\\necart type est {ecart_type}\\nle seuil est {threshold}')\n",
        "  return list_phone_number_user, sum_operation, mini, maxi, mediane, moyenne, ecart_type, threshold"
      ],
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1Cj2HoAlhdO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "7w_3sw2eFVx5",
        "outputId": "6faa710d-8079-42f8-b0a3-df80b92f907b"
      },
      "source": [
        "list_phone_number_user_client, sum_operation_client, min_client, max_client, mediane_client, moyenne_client, et_client, threshold_client = calculate_metric(all_df_list_new)\n",
        "list_phone_number_user_decline, sum_operation_decline, mini_decline, maxi_decline, mediane_decline, moyenne_decline, et_decline, threshold_decline = calculate_metric(all_df_list_new, str_key_word='/decline/')\n",
        "list_phone_number_user_lifestyle, sum_operation_lifestyle, min_lifestyle, max_lifestyle, mediane_lifestyle, moyenne_lifestyle, et_lifestyle, threshold_lifestyle = calculate_metric(all_df_list_new, str_key_word='lifestyle')\n",
        "list_phone_number_user_payment, sum_operation_payment, min_payment, max_payment, mediane_payment, moyenne_payment, et_payment, threshold_payment = calculate_metric(all_df_list_new, str_key_word='payment')\n",
        "list_phone_number_user_app_get, sum_operation_app_get, min_app_get, max_app_get, mediane_app_get, moyenne_app_get, et_app_get, threshold_app_get = calculate_metric(all_df_list_new, str_key_word='applications/get')\n",
        "list_phone_number_user_preapproved, sum_operation_preapproved, min_preapproved, max_preapproved, mediane_preapproved, moyenne_preapproved, et_preapproved, threshold_preapproved = calculate_metric(all_df_list_new, str_key_word='preapproved')\n",
        "list_phone_number_user_accept, sum_operation_accept, min_accept, max_accept, mediane_accept, moyenne_accept, et_accept, threshold_accept = calculate_metric(all_df_list_new, str_key_word='accept')\n",
        "list_phone_number_user_wallet_balance, sum_operation_wallet_balance, min_wallet_balance, max_wallet_balance, mediane_wallet_balance, moyenne_wallet_balance, et_wallet_balance, threshold_wallet_balance = calculate_metric(all_df_list_new, str_key_word='wallet/balance')\n",
        "list_phone_number_user_adverts, sum_operation_adverts, min_adverts, max_adverts, mediane_adverts, moyenne_adverts, et_adverts, threshold_adverts = calculate_metric(all_df_list_new, str_key_word='adverts')\n",
        "list_phone_number_user_bank, sum_operation_bank, min_bank, max_bank, mediane_bank, moyenne_bank, et_bank, threshold_bank = calculate_metric(all_df_list_new, str_key_word='bank')\n",
        "list_phone_number_user_update, sum_operation_update, min_update, max_update, mediane_update, moyenne_update, et_update, threshold_update = calculate_metric(all_df_list_new, str_key_word='update')\n",
        "list_phone_number_user_product, sum_operation_product, min_product, max_product, mediane_product, moyenne_product, et_product, threshold_product = calculate_metric(all_df_list_new, str_key_word='products')\n",
        "list_phone_number_user_location, sum_operation_Location, min_Location, max_Location, mediane_Location, moyenne_Location, et_Location, threshold_Location = calculate_metric(all_df_list_new, str_key_word='locations')\n",
        "list_phone_number_user_client_get, sum_operation_client_get, min_client_get, max_client_get, mediane_client_get, moyenne_client_get, et_client_get, threshold_client_get = calculate_metric(all_df_list_new, str_key_word='client/get')\n",
        "list_phone_number_user_client_v2_create, sum_operation_client_v2_create, min_client_v2_create, max_client_v2_create, mediane_client_v2_create, moyenne_client_v2_create, et_client_v2_create, threshold_client_v2_create = calculate_metric(all_df_list_new, str_key_word='client/v2/create')\n"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-131-f601d93ef619>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist_phone_number_user_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_operation_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmediane_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoyenne_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0met_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_df_list_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlist_phone_number_user_decline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_operation_decline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_decline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxi_decline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmediane_decline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoyenne_decline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0met_decline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold_decline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_df_list_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_key_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/decline/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlist_phone_number_user_lifestyle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_operation_lifestyle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_lifestyle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lifestyle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmediane_lifestyle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoyenne_lifestyle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0met_lifestyle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold_lifestyle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_df_list_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_key_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lifestyle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlist_phone_number_user_payment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_operation_payment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_payment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_payment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmediane_payment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoyenne_payment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0met_payment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold_payment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_df_list_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_key_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'payment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlist_phone_number_user_app_get\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_operation_app_get\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_app_get\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_app_get\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmediane_app_get\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoyenne_app_get\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0met_app_get\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold_app_get\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_df_list_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_key_word\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'applications/get'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'all_df_list_new' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtWTF7J2iTes"
      },
      "source": [
        "### Plot des utilisateurs en fonctions des opérations effectuées."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZ2fonVBfGpY"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def plot_users(list_operations, title_operation='lifestyle', threshold=threshold_lifestyle):\n",
        "  bins = np.linspace(math.ceil(min(list_operations)), \n",
        "                    math.floor(max(list_operations)),\n",
        "                    10) # fixed number of bins\n",
        "  plt.axvline(x=threshold, color='red') # pour tracer le threshold \n",
        "  plt.xlim([min(list_operations)-5, max(list_operations)+5])\n",
        "\n",
        "  plt.hist(list_operations, bins=bins, alpha=0.5)\n",
        "  plt.title(f'Distribution des operations ({title_operation}) avec seuil {threshold}')\n",
        "  plt.xlabel(f'somme des opérations {title_operation}')\n",
        "  plt.ylabel('count')\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouk_FV29kexj"
      },
      "source": [
        "### si les données sont à la moyenne plus 3 écart-types alors, il y'a de fortes chances qu'elles soient des outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM_m15rGfsQe"
      },
      "source": [
        "plot_users(sum_operation_lifestyle, 'lifestyle', threshold_lifestyle)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4kGtEsYgOFM"
      },
      "source": [
        "plot_users(sum_operation_accept, 'accept', threshold_accept)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Klg_5iXgWIg"
      },
      "source": [
        "plot_users(sum_operation_adverts, 'adverts', threshold_adverts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsIWqMjHgmuG"
      },
      "source": [
        "plot_users(sum_operation_app_get, 'app_get', threshold_app_get)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYn60wcQgtla"
      },
      "source": [
        "plot_users(sum_operation_bank, 'bank', threshold_bank)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YJnUP1ag0Tu"
      },
      "source": [
        "plot_users(sum_operation_client, 'client', threshold_client)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InadmuwghGG_"
      },
      "source": [
        "plot_users(sum_operation_client_get, 'client_get', threshold_client_get)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVgVbFeXhMq1"
      },
      "source": [
        "plot_users(sum_operation_client_v2_create, 'client_v2_create', threshold_client_v2_create)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znS6VKUChUBL"
      },
      "source": [
        "plot_users(sum_operation_decline, 'decline', threshold_decline)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_ft5LZdhc4y"
      },
      "source": [
        "plot_users(sum_operation_Location, 'locations', threshold_Location)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs7egji5hi2i"
      },
      "source": [
        "plot_users(sum_operation_payment, 'payment', threshold_payment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XfMMIQmhsnr"
      },
      "source": [
        "plot_users(sum_operation_preapproved, 'preapproved', threshold_preapproved)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEE7rnaghyL1"
      },
      "source": [
        "plot_users(sum_operation_product, 'products', threshold_product)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa8kAA6Sh9Bb"
      },
      "source": [
        "plot_users(sum_operation_update, 'updated', threshold_update)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTXF2b78iCSJ"
      },
      "source": [
        "plot_users(sum_operation_wallet_balance, 'wallet_balance', threshold_wallet_balance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnH84tAT32yR"
      },
      "source": [
        "### Regrouper les utilisateurs en fonction du seuil"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7gOEGHq3Uad"
      },
      "source": [
        "def group_users(list_users, list_phone_number_user, threshold):\n",
        "  '''cette fonction permet de regrouper les utilisateurs\n",
        "    en fonction de si leurs nombres d'opérations sont supérieurs ou inférieurs\n",
        "    à un seuil.  '''\n",
        "\n",
        "  list_users_great_than_threshold = []\n",
        "  list_users_less_than_threshold = []\n",
        "  dict_res = {}\n",
        "\n",
        "  for i, element in enumerate(list_users):\n",
        "    if element > threshold:\n",
        "      dict_res[list_phone_number_user[i]] = 1\n",
        "      list_users_great_than_threshold.append(list_phone_number_user[i])\n",
        "    elif element < threshold:\n",
        "      dict_res[list_phone_number_user[i]] = 0\n",
        "      list_users_less_than_threshold.append(list_phone_number_user[i])\n",
        "  \n",
        "  return dict_res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn_LH5Ha3apC"
      },
      "source": [
        "dict_client = group_users(sum_operation_client, list_phone_number_user_client, threshold_client)\n",
        "dict_accept = group_users(sum_operation_accept, list_phone_number_user_accept, threshold_accept)\n",
        "dict_adverts = group_users(sum_operation_adverts, list_phone_number_user_adverts, threshold_adverts)\n",
        "dict_app_get = group_users(sum_operation_app_get, list_phone_number_user_app_get, threshold_app_get)\n",
        "dict_bank = group_users(sum_operation_bank, list_phone_number_user_bank, threshold_bank)\n",
        "dict_client_get = group_users(sum_operation_client_get, list_phone_number_user_client_get, threshold_client_get)\n",
        "dict_client_v2_create = group_users(sum_operation_client_v2_create, list_phone_number_user_client_v2_create, threshold_client_v2_create)\n",
        "dict_decline = group_users(sum_operation_decline, list_phone_number_user_decline, threshold_decline)\n",
        "dict_lifestyle = group_users(sum_operation_lifestyle, list_phone_number_user_lifestyle, threshold_lifestyle)\n",
        "dict_location = group_users(sum_operation_Location, list_phone_number_user_location, threshold_Location)\n",
        "dict_payment = group_users(sum_operation_payment, list_phone_number_user_payment, threshold_payment)\n",
        "dict_preapproved = group_users(sum_operation_preapproved, list_phone_number_user_preapproved, threshold_preapproved)\n",
        "dict_product = group_users(sum_operation_product, list_phone_number_user_product, threshold_product)\n",
        "dict_update = group_users(sum_operation_update, list_phone_number_user_update, threshold_update)\n",
        "dict_wallet_balance = group_users(sum_operation_wallet_balance, list_phone_number_user_wallet_balance, threshold_wallet_balance)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYiYbc4o3gwk"
      },
      "source": [
        "### Créer des DataFrame en fonction des utilisateurs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc7L7Uyh3hZk"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def create_df(one_dict, key_word='great_less_than_threshold'):\n",
        "\n",
        "  list_key = []\n",
        "  list_value = []\n",
        "  for key in one_dict:\n",
        "    list_key.append(key)\n",
        "    list_value.append(one_dict[key])\n",
        "\n",
        "  dict_res ={'phone_number_user': list_key, key_word: list_value}\n",
        "  return pd.DataFrame(dict_res)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2RnLonUW5Kr"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de endpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzMRYT9z3uNC"
      },
      "source": [
        "# Création du dataframe en fonction de l'utilisation du threshold de l'endpoint \"client\"\n",
        "df_client=create_df(dict_client, key_word='great_less_than_threshold_client')\n",
        "df_client"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCQ_hPjLNtB3"
      },
      "source": [
        "# vérifions les utilisateurs qui ont un seuil = 1\n",
        "df_client[df_client['great_less_than_threshold_client'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuGTWziGXKj6"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"accept\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0FsYHuU6Gsf"
      },
      "source": [
        "df_accept = create_df(dict_accept, key_word='great_less_than_threshold_accept')\n",
        "df_accept"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34g-7z8rN-jF"
      },
      "source": [
        "df_accept[df_accept['great_less_than_threshold_accept'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNfysTTjXQNN"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"adverts\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUzR3w9_6GoH"
      },
      "source": [
        "df_adverts = create_df(dict_adverts, key_word='great_less_than_threshold_adverts')\n",
        "df_adverts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWjqjk0YODEp"
      },
      "source": [
        "df_adverts[df_adverts['great_less_than_threshold_adverts'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PT1LsxzsXTMQ"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"application get\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfeKkHnb6Y71"
      },
      "source": [
        "df_app_get = create_df(dict_app_get, key_word='great_less_than_threshold_app_get')\n",
        "df_app_get"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGNCTqUWOJN3"
      },
      "source": [
        "df_app_get[df_app_get['great_less_than_threshold_app_get'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikjO7KW6XXHg"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"bank\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjTg3xcT6Gfe"
      },
      "source": [
        "df_bank = create_df(dict_bank, key_word='great_less_than_threshold_bank')\n",
        "df_bank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjKmqTMyOO-B"
      },
      "source": [
        "df_bank[df_bank['great_less_than_threshold_bank'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho_DePm4XaPh"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"client get\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FRYmeN-6nht"
      },
      "source": [
        "df_client_get = create_df(dict_client_get, key_word='great_less_than_threshold_client_get')\n",
        "df_client_get"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CibNVa4qOUSF"
      },
      "source": [
        "df_client_get[df_client_get['great_less_than_threshold_client_get'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30jyHh6sXduX"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"client v2 create\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL485DPM6nZl"
      },
      "source": [
        "df_client_v2_create = create_df(dict_client_v2_create, key_word='great_less_than_threshold_client_v2_create')\n",
        "df_client_v2_create"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SShqEPlOZ6l"
      },
      "source": [
        "df_client_v2_create[df_client_v2_create['great_less_than_threshold_client_v2_create'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKL3d9m9XhJE"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"decline\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7f4_nTjf6nN2"
      },
      "source": [
        "df_decline = create_df(dict_decline, key_word='great_less_than_threshold_decline')\n",
        "df_decline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKbuK32BOjB1"
      },
      "source": [
        "df_decline[df_decline['great_less_than_threshold_decline'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTzb6FhiXj4I"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"payment\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02wPdTJo6mhQ"
      },
      "source": [
        "df_payment = create_df(dict_payment, key_word='great_less_than_threshold_payment')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNah0SmGOn8U"
      },
      "source": [
        "df_payment[df_payment['great_less_than_threshold_payment'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssC1iZZSXmBt"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"lifestyle\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofw4nTND7KkF"
      },
      "source": [
        "df_lifestyle = create_df(dict_lifestyle, key_word='great_less_than_threshold_lifestyle')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsJEC3NxOuG7"
      },
      "source": [
        "df_lifestyle[df_lifestyle['great_less_than_threshold_lifestyle'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRc0_bXoXowm"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"location\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QuRG71Q7Kgo"
      },
      "source": [
        "df_location = create_df(dict_location, key_word='great_less_than_threshold_location')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uujNGnO_O0jo"
      },
      "source": [
        "df_location[df_location['great_less_than_threshold_location'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnTnM2eiXryg"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"preapproved\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyErZU8Y7KdN"
      },
      "source": [
        "df_preapproved = create_df(dict_preapproved, key_word='great_less_than_threshold_preapproved')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLSoJAzqO9ks"
      },
      "source": [
        "df_preapproved[df_preapproved['great_less_than_threshold_preapproved'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J38A8gCSXwZ2"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"products\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBsjJRN57gx6"
      },
      "source": [
        "df_product = create_df(dict_product, key_word='great_less_than_threshold_products')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgxI-YLLPOsl"
      },
      "source": [
        "df_product[df_product['great_less_than_threshold_products'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfUEW3UBXyzI"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"update\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTbqyzDv7hJG"
      },
      "source": [
        "df_update = create_df(dict_update, key_word='great_less_than_threshold_update')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qds06n3CPUzN"
      },
      "source": [
        "df_update[df_update['great_less_than_threshold_update'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7H2G48kX1k_"
      },
      "source": [
        "### création du dataframe pour les utilisateurs qui ont effectué les opérations de \"wallet balance\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jH3HRzy7pHX"
      },
      "source": [
        "df_wallet_balance = create_df(dict_wallet_balance, key_word='great_less_than_threshold_wallet_balance')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn6owJrOPhpT"
      },
      "source": [
        "df_wallet_balance[df_wallet_balance['great_less_than_threshold_wallet_balance'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2pQUs8EYAgz"
      },
      "source": [
        "### Faire la jointure des dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBvKDguMaR4_"
      },
      "source": [
        "### premièrement: ordonner les colonnes de numéros de téléphone par ordre croissant\n",
        "### NB: Toujours re-indexer (.reset_index(drop=True)) les dataframe avant de faire des JOIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9mMrkjRaakK"
      },
      "source": [
        "# Ordonnons les dataset, ensuite ré-indexons les index ce qui nous facilitera l'opération de concatenation.\n",
        "sorted_df_client = df_client.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_accept = df_accept.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_adverts = df_adverts.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_app_get = df_app_get.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_bank = df_bank.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_client_get = df_client_get.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_client_v2_create = df_client_v2_create.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_decline = df_decline.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_payment = df_payment.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_lifestyle = df_lifestyle.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_location = df_location.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_preapproved = df_preapproved.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_product = df_product.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_update = df_update.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)\n",
        "sorted_df_wallet_balance = df_wallet_balance.sort_values(by=['phone_number_user'], ascending=True).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RxuKZqm5i_6"
      },
      "source": [
        "### Concatenons tous les dataframes pour avoir un dataframe avec lequel on va entrainer nos modèles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLeEFCNdc1Zv"
      },
      "source": [
        "list_users = [sorted_df_client, sorted_df_accept['great_less_than_threshold_accept'], sorted_df_adverts['great_less_than_threshold_adverts'], sorted_df_app_get['great_less_than_threshold_app_get'], \n",
        "              sorted_df_bank['great_less_than_threshold_bank'], sorted_df_client_get['great_less_than_threshold_client_get'], \n",
        "              sorted_df_client_v2_create['great_less_than_threshold_client_v2_create'], sorted_df_decline['great_less_than_threshold_decline'], \n",
        "              sorted_df_payment['great_less_than_threshold_payment'], sorted_df_lifestyle['great_less_than_threshold_lifestyle'], \n",
        "              sorted_df_location['great_less_than_threshold_location'], sorted_df_preapproved['great_less_than_threshold_preapproved'], \n",
        "              sorted_df_product['great_less_than_threshold_products'], sorted_df_update['great_less_than_threshold_update'], \n",
        "              sorted_df_wallet_balance['great_less_than_threshold_wallet_balance']]  # List of our dataframes\n",
        "df_final_users = pd.concat(list_users, axis=1)\n",
        "df_final_users.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mloO7X6ngHQ9"
      },
      "source": [
        "# Faire la somme de toutes les colonnes et mettre dans le résultat dans la colonne \"score_total\"\n",
        "df_final_users['score_total'] = df_final_users.iloc[:, 1:16].sum(axis=1)\n",
        "df_final_users['score_total'].max()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVjWnukpmO_Z"
      },
      "source": [
        "list_score_total = df_final_users['score_total'].values # convertir une series en list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwwCndmYmiZF"
      },
      "source": [
        "list_score_total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1IG8mJfmnkW"
      },
      "source": [
        "import numpy as np\n",
        "med = np.median(list_score_total)\n",
        "moy = np.mean(list_score_total)\n",
        "ecart_type_score_total = np.std(list_score_total)\n",
        "threshold_score_total = med + 3*ecart_type_score_total\n",
        "threshold_score_total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ysT-1E5ieco"
      },
      "source": [
        "plot_users(df_final_users['score_total'].values, 'score_total' , threshold_score_total)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCfvCQFYoRId"
      },
      "source": [
        "### Récupérer les utilisateurs qui sont au delà du seuil.\n",
        "### les utilisateurs qui sont au délà du seuil peuvent etre considérés comme ceux qui ont effectué des transactions douteuses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mh2KMJenpUa"
      },
      "source": [
        "df_resultats = df_final_users[df_final_users['score_total'] > threshold_score_total]\n",
        "df_resultats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQoJDhPCu-6_"
      },
      "source": [
        "df_final_users"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aH7MEk4zClL"
      },
      "source": [
        "### Selon les résultats affichés, la composantes pc2 de PCA répresente les personnes qui ont effectués beaucoup d'achats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9IwZ8Igx-Sn"
      },
      "source": [
        " # afficher les lignes 13 et 32 du dataframe\n",
        " df_final_users.loc[[13,32],:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEPa2KgIWfXD"
      },
      "source": [
        "# afficher la ligne 54 du dataframe\n",
        " df_final_users.loc[[54],:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTF-NKnQxlsG"
      },
      "source": [
        "### on drop la prémière colonne parce que les données qu'on veut passer à notre modèle de clustering doivent etre des nombres"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRLmJDdatpmP"
      },
      "source": [
        "df_result = df_final_users.drop('phone_number_user', 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOVcjrWmxzje"
      },
      "source": [
        "### ensuite on convertit notre dataframe en liste pour le passer au  modèle de clustering KMeans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9H9zAdOuvV8"
      },
      "source": [
        "# convertir le dataframe en list\n",
        "list_resultats = df_result.values.tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaF3aVLWpjv4"
      },
      "source": [
        "### Algorithme de clustering KMeans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0HBS_T5pz4O"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz3JkqhpbTML"
      },
      "source": [
        "### TODO PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbX0vDx1fEpp"
      },
      "source": [
        "### on fait la standardisation parce que la dernière colonne du dataframe a des valeurs prépondérantes (9, 3, etc...) par rapport aux autres pour que toutes les valeurs du dataframe soient comprises entre -1 et 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMGNKQbLbqb_"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler \n",
        "Scaled_data = StandardScaler().fit_transform(list_resultats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzyRZKHPp8VN"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import random\n",
        "\n",
        "# Scale data before applying PCA\n",
        "#scaling=StandardScaler()\n",
        " \n",
        "# Use fit and transform method\n",
        "#scaling.fit(df1)\n",
        "#Scaled_data=scaling.transform(df1)\n",
        " \n",
        "# Set the n_components=2\n",
        "principal=PCA(n_components=2)\n",
        "principal.fit(Scaled_data)\n",
        "x=principal.transform(Scaled_data)\n",
        " \n",
        "# Check the dimensions of data after PCA\n",
        "#print(x.shape)\n",
        "\n",
        "principal.components_\n",
        "\n",
        "df_sortie = pd.DataFrame(x, columns=['pc1', 'pc2'])\n",
        "#print(df_sortie)\n",
        "df_filtre = df_sortie[(df_sortie['pc1'] >= 7) & (df_sortie['pc2'] >= 6)]\n",
        "print(df_filtre)\n",
        "\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(x[:,0]+random.uniform(0, 1)/10, x[:,1]+random.uniform(0, 1)/100, c='blue', cmap='plasma', marker='x', alpha=0.4)\n",
        "plt.xlabel('pc1')\n",
        "plt.ylabel('pc2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eVSjcgLWODF"
      },
      "source": [
        "df_filtre_1 = df_sortie[(df_sortie['pc1'] <= 5) & (df_sortie['pc2'] < -1)]\n",
        "print(df_filtre_1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5A2uZHfpSc7"
      },
      "source": [
        "### Fonction pour faire le plot de tous nos points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9SGppTGn36_"
      },
      "source": [
        "def scatter_plot(list_points):\n",
        "  x = []\n",
        "  y = []\n",
        "  for i in list_points:\n",
        "    x.append(i[0])\n",
        "    y.append(i[1])\n",
        "\n",
        "  plt.scatter(x,y, s=200)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luswXN-Gn5K2"
      },
      "source": [
        "scatter_plot(list_resultats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PErKtm2osUs_"
      },
      "source": [
        "# on choisit trois clusters\n",
        "kmeans = KMeans(n_clusters=3, random_state=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_zFNTyBEZX8"
      },
      "source": [
        "y_pred = kmeans.fit_predict(list_resultats)\n",
        "y_pred\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP9dBz3pHKA8"
      },
      "source": [
        "# pour avoir la position finale de nos centroides\n",
        "clusters = kmeans.cluster_centers_\n",
        "# comme on peut voir dans le résultat, on a comme coordonnées de nos centroides,\n",
        "# on a un tableau de 3 lignes et 16 colonnes\n",
        "clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SfD0vFopdWl"
      },
      "source": [
        "x = []\n",
        "y = []\n",
        "for i in list_resultats:\n",
        "  x.append(i[0])\n",
        "  y.append(i[1])\n",
        "\n",
        "plt.scatter(x, y, c=y_pred, s=200)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E33vBlFupo_u"
      },
      "source": [
        "# on affiche tous nos centroides pour l'axe 0 et tous nos centroides pour l'axe 1\n",
        "plt.scatter(clusters[:,0], clusters[:,1], c='red')\n",
        "# c'est grace à ces centroides qu'on va pouvoir faire nos futures prédictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0tVFAiDso1Y"
      },
      "source": [
        "# calculons la fonction cout pour notre modèle\n",
        "# c'est la somme des distances entre un cluster et le centroide \n",
        "kmeans.score(list_resultats)\n",
        "#kmeans.inertia_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nttqtNbHuy2S"
      },
      "source": [
        "### essayons de chercher le nombre de cluster pour notre algorithme"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAkmnEScsoXU"
      },
      "source": [
        "inertia = []\n",
        "k_range = range(1, 20)\n",
        "for k in k_range:\n",
        "  model = KMeans(n_clusters=k).fit(list_resultats)\n",
        "  inertia.append(model.inertia_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-9Bc6hvv8zx"
      },
      "source": [
        "plt.plot(k_range, inertia)\n",
        "plt.xlabel('Nombre de clusters')\n",
        "plt.ylabel('Cout du model(Inertia)')\n",
        "# selon le graphique ci-dessous, la zone de coude correspond à 2 clusters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrLNrXS2g21D"
      },
      "source": [
        "### Test de silhouette qui aide à départager si on a 2 ou 3 clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbhrcutA0byb"
      },
      "source": [
        "###Autre technique pour detecter les anomalies (Isolation Forest)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylGkXZNT0jvY"
      },
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "#au modèle, on indique qu'on a par exemple 2% des données de notre dataset sont contaminés\n",
        "model = IsolationForest(contamination=0.02)\n",
        "model.fit(list_resultats)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5flMdqk2RpO"
      },
      "source": [
        "# quand on fait notre prédiction, les 1 répresentent les données qui ne sont pas des anomalies\n",
        "# et les -1 répresentent les données avec anomalies. \n",
        "list_pred = model.predict(list_resultats)\n",
        "# pour les filtrer et les afficher, on va faire le boolean indexing\n",
        "# maintenant on doit convertir cette liste (list_pred) en une colonne et la concatener au dataframe pour avoir\n",
        "# son correspondant\n",
        "list_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdZS4UCqjhur"
      },
      "source": [
        "df_final_users['prediction_Iso_Forest'] = pd.Series(list_pred) # ajout de la prédiction de IsolationForest dans le dataframe.\n",
        "df_final_users['prediction_KMeans'] = pd.Series(y_pred) # ajout de la prediction de KMeans dans le dataframe\n",
        "\n",
        "df_final_users"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDfM_7X6iMth"
      },
      "source": [
        "### il faudrait savoir si ce modèle a bien prédit et pour cela il faudrait confronter ces données avec ce qu'on a dans le dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rxt47ASS3Ltd"
      },
      "source": [
        "# on cherche ceux qui sont égalent à -1\n",
        "outliers = model.predict(list_resultats) == -1\n",
        "outliers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eE62grtb1coO"
      },
      "source": [
        "x = []\n",
        "y = []\n",
        "for i in list_resultats:\n",
        "  x.append(i[0])\n",
        "  y.append(i[1])\n",
        "\n",
        "plt.scatter(x, y, c=model.predict(list_resultats), s=200)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhOwNRVI-jrd"
      },
      "source": [
        "### DBScan"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKHG2HF--m-2"
      },
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "dbscan = DBSCAN(eps=2, min_samples=3).fit(list_resultats)\n",
        "clusters_dbscan = dbscan.labels_ # selon ce resultat, on 2 clusters\n",
        "clusters_dbscan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQkFPQtqqd4d"
      },
      "source": [
        "df_final_users['prediction_dbscan'] = pd.Series(clusters_dbscan) # ajout de la prediction de DBSCAN dans le dataframe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KK_O7ijOhadP"
      },
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "silhouette_score(list_resultats, clusters_dbscan)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DD0yWEBtbDpF"
      },
      "source": [
        "### Choisir la valeur optimale de eps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NowVvyWDbIQz"
      },
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "neigh = NearestNeighbors(n_neighbors=2)\n",
        "nbrs = neigh.fit(list_resultats)\n",
        "distances, indices = nbrs.kneighbors(list_resultats) # distance est la distance entre 2 points\n",
        "distances = np.sort(distances, axis=0)\n",
        "distances = distances[:,1]\n",
        "plt.plot(distances)\n",
        "# quand on trace les 115 points "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re9boV6he8co"
      },
      "source": [
        "indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDHg4A6Nkm5B"
      },
      "source": [
        "### Hierarchical Clustering (Agglomerative Clustering)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2JAY6yjkyI6"
      },
      "source": [
        "from sklearn.cluster import AgglomerativeClustering\n",
        "import scipy.cluster.hierarchy as sch\n",
        "\n",
        "# créons un dendogramme \n",
        "dendogram = sch.dendrogram(sch.linkage(list_resultats, method='ward'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ju9rBl5dn2xW"
      },
      "source": [
        "# effectuons le clustering\n",
        "hc = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n",
        "# mettre chaque data point dans un cluster\n",
        "y_hc = hc.fit_predict(list_resultats)\n",
        "y_hc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anAvQ2KAqmi8"
      },
      "source": [
        "df_final_users['prediction_agglomerativeclustering'] = pd.Series(y_hc) # ajout de la prediction de Agglomerative dans le dataframe\n",
        "df_final_users"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}